{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, log_loss, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D,Conv2D,MaxPooling2D,Activation,Dropout\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.attacks.evasion import FastGradientMethod, CarliniLInfMethod, DeepFool, ProjectedGradientDescent\n",
    "from art.attacks.evasion import FeatureAdversariesTensorFlowV2\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "if tf.__version__[0] != '2':\n",
    "    raise ImportError('This notebook requires TensorFlow v2.')\n",
    "tf.compat.v1.disable_eager_execution()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T17:06:44.097853Z",
     "end_time": "2023-05-28T17:06:53.047982Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1140, 62, 47)\n",
      "(1140, 2914)\n",
      "(5,)\n",
      "(1140,)\n",
      "['Colin Powell' 'Donald Rumsfeld' 'George W Bush' 'Gerhard Schroeder'\n",
      " 'Tony Blair']\n",
      "5\n",
      "(1140, 62, 47)\n",
      "(1140, 62, 47, 1)\n",
      "(160, 62, 47, 1)\n",
      "(41, 62, 47, 1)\n",
      "(160, 5)\n"
     ]
    }
   ],
   "source": [
    "lfw_dataset=fetch_lfw_people(min_faces_per_person=100)\n",
    "\n",
    "print(lfw_dataset.images.shape)\n",
    "print(lfw_dataset.data.shape)\n",
    "print(lfw_dataset.target_names.shape)\n",
    "print(lfw_dataset.target.shape)\n",
    "\n",
    "lfw_dataset.target\n",
    "\n",
    "Name = lfw_dataset.target_names\n",
    "\n",
    "print(Name)\n",
    "print(len(Name))\n",
    "\n",
    "N=[]\n",
    "for i in range(len(Name)):\n",
    "    N+=[i]\n",
    "\n",
    "mapping=dict(zip(Name,N))\n",
    "reverse_mapping=dict(zip(N,Name))\n",
    "\n",
    "def mapper(value):\n",
    "    return reverse_mapping[value]\n",
    "\n",
    "X0=lfw_dataset.images\n",
    "y=lfw_dataset.target\n",
    "\n",
    "print(X0.shape)\n",
    "X=X0.reshape(-1,62,47,1)\n",
    "print(X.shape)\n",
    "\n",
    "dataset=[]\n",
    "testset=[]\n",
    "t=0\n",
    "for Xi,yi in zip(X,y):\n",
    "    img=Xi/255.0\n",
    "    if t<=200:\n",
    "        dataset.append([img,yi])\n",
    "    else:\n",
    "        testset.append([img,yi])\n",
    "    t+=1\n",
    "\n",
    "data,labels0=zip(*dataset)\n",
    "test,tlabels0=zip(*testset)\n",
    "\n",
    "labels1=to_categorical(labels0)\n",
    "data=np.array(data)\n",
    "labels=np.array(labels1)\n",
    "\n",
    "tlabels1=to_categorical(tlabels0)\n",
    "test=np.array(test)\n",
    "tlabels=np.array(tlabels1)\n",
    "\n",
    "# trainx,testx,trainy,testy=train_test_split(data,labels,test_size=0.2,random_state=44)\n",
    "trainx,testx,trainy,testy=train_test_split(data,labels,test_size=0.2,random_state=1)\n",
    "\n",
    "x_train = trainx.astype(np.float32)\n",
    "x_test = testx.astype(np.float32)\n",
    "y_train = trainy.astype(np.float32)\n",
    "y_test = testy.astype(np.float32)\n",
    "\n",
    "print(trainx.shape)\n",
    "print(testx.shape)\n",
    "print(trainy.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T17:06:52.999381Z",
     "end_time": "2023-05-28T17:06:53.134554Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, h=62, w=47, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    # plt.figure.patch.set_facecolor('white')\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i+1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(Name[np.argmax(titles[i])], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "def plot_image(image, title):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.matshow(image, cmap=plt.cm.gray)\n",
    "    plt.title(Name[np.argmax(title)], size=12)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T17:06:53.104470Z",
     "end_time": "2023-05-28T17:06:53.134856Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 60, 45, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 30, 22, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 20, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 10, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4480)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               2294272   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,370,149\n",
      "Trainable params: 2,370,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from art.attacks.evasion import FeatureAdversariesTensorFlowV2\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.utils import load_mnist\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32,(3,3), input_shape=(62,47,1), activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(32,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Step 2a: Define the loss function and optimizer\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def train_step(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "# model = keras.models.load_model(\"VGG_model_87.pth\")\n",
    "# datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rotation_range=20,zoom_range=0.2,\n",
    "#                     width_shift_range=0.2,height_shift_range=0.2,shear_range=0.1,fill_mode=\"nearest\")\n",
    "#\n",
    "# # model.fit(trainx,trainy, validation_data=(testx,testy),epochs=100, batch_size=32)\n",
    "# his=model.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T17:06:53.111866Z",
     "end_time": "2023-05-28T17:06:53.279642Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 41 samples\n",
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 17:06:53.386568: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-28 17:06:53.387043: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-05-28 17:06:53.406277: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-28 17:06:53.406878: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-28 17:06:53.443484: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 17:06:53.649540: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-28 17:06:53.656812: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 1s 4ms/sample - loss: 1.5247 - accuracy: 0.2938 - val_loss: 1.5093 - val_accuracy: 0.4390\n",
      "Epoch 2/300\n",
      "128/160 [=======================>......] - ETA: 0s - loss: 1.4794 - accuracy: 0.4844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 17:06:54.227003: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 925us/sample - loss: 1.4816 - accuracy: 0.4750 - val_loss: 1.4862 - val_accuracy: 0.4390\n",
      "Epoch 3/300\n",
      "160/160 [==============================] - 0s 532us/sample - loss: 1.3946 - accuracy: 0.4750 - val_loss: 1.4644 - val_accuracy: 0.4390\n",
      "Epoch 4/300\n",
      "160/160 [==============================] - 0s 687us/sample - loss: 1.3967 - accuracy: 0.4750 - val_loss: 1.4374 - val_accuracy: 0.4390\n",
      "Epoch 5/300\n",
      "160/160 [==============================] - 0s 910us/sample - loss: 1.3527 - accuracy: 0.4750 - val_loss: 1.5032 - val_accuracy: 0.4390\n",
      "Epoch 6/300\n",
      "160/160 [==============================] - 0s 831us/sample - loss: 1.3791 - accuracy: 0.4750 - val_loss: 1.4613 - val_accuracy: 0.4390\n",
      "Epoch 7/300\n",
      "160/160 [==============================] - 0s 787us/sample - loss: 1.3364 - accuracy: 0.4750 - val_loss: 1.4222 - val_accuracy: 0.4390\n",
      "Epoch 8/300\n",
      "160/160 [==============================] - 0s 756us/sample - loss: 1.3282 - accuracy: 0.4750 - val_loss: 1.4081 - val_accuracy: 0.4390\n",
      "Epoch 9/300\n",
      "160/160 [==============================] - 0s 733us/sample - loss: 1.3046 - accuracy: 0.4750 - val_loss: 1.4021 - val_accuracy: 0.4390\n",
      "Epoch 10/300\n",
      "160/160 [==============================] - 0s 737us/sample - loss: 1.2680 - accuracy: 0.4750 - val_loss: 1.3769 - val_accuracy: 0.4390\n",
      "Epoch 11/300\n",
      "160/160 [==============================] - 0s 977us/sample - loss: 1.2373 - accuracy: 0.5188 - val_loss: 1.3434 - val_accuracy: 0.4878\n",
      "Epoch 12/300\n",
      "160/160 [==============================] - 0s 843us/sample - loss: 1.1739 - accuracy: 0.5250 - val_loss: 1.2910 - val_accuracy: 0.5366\n",
      "Epoch 13/300\n",
      "160/160 [==============================] - 0s 774us/sample - loss: 1.1359 - accuracy: 0.6125 - val_loss: 1.2374 - val_accuracy: 0.5366\n",
      "Epoch 14/300\n",
      "160/160 [==============================] - 0s 754us/sample - loss: 1.0500 - accuracy: 0.6313 - val_loss: 1.2386 - val_accuracy: 0.5366\n",
      "Epoch 15/300\n",
      "160/160 [==============================] - 0s 772us/sample - loss: 0.9913 - accuracy: 0.6188 - val_loss: 1.1178 - val_accuracy: 0.5366\n",
      "Epoch 16/300\n",
      "160/160 [==============================] - 0s 805us/sample - loss: 0.9133 - accuracy: 0.6687 - val_loss: 1.0151 - val_accuracy: 0.6341\n",
      "Epoch 17/300\n",
      "160/160 [==============================] - 0s 757us/sample - loss: 0.8232 - accuracy: 0.7563 - val_loss: 1.0844 - val_accuracy: 0.5366\n",
      "Epoch 18/300\n",
      "160/160 [==============================] - 0s 749us/sample - loss: 0.8093 - accuracy: 0.7375 - val_loss: 1.0121 - val_accuracy: 0.5366\n",
      "Epoch 19/300\n",
      "160/160 [==============================] - 0s 790us/sample - loss: 0.7027 - accuracy: 0.7063 - val_loss: 0.9008 - val_accuracy: 0.6829\n",
      "Epoch 20/300\n",
      "160/160 [==============================] - 0s 764us/sample - loss: 0.6020 - accuracy: 0.8500 - val_loss: 0.9727 - val_accuracy: 0.5366\n",
      "Epoch 21/300\n",
      "160/160 [==============================] - 0s 750us/sample - loss: 0.5879 - accuracy: 0.7688 - val_loss: 0.7759 - val_accuracy: 0.8049\n",
      "Epoch 22/300\n",
      "160/160 [==============================] - 0s 741us/sample - loss: 0.5021 - accuracy: 0.9062 - val_loss: 1.0277 - val_accuracy: 0.5366\n",
      "Epoch 23/300\n",
      "160/160 [==============================] - 0s 744us/sample - loss: 0.4621 - accuracy: 0.8313 - val_loss: 0.7915 - val_accuracy: 0.6341\n",
      "Epoch 24/300\n",
      "160/160 [==============================] - 0s 738us/sample - loss: 0.3437 - accuracy: 0.9313 - val_loss: 0.7051 - val_accuracy: 0.7317\n",
      "Epoch 25/300\n",
      "160/160 [==============================] - 0s 899us/sample - loss: 0.2956 - accuracy: 0.9187 - val_loss: 0.7654 - val_accuracy: 0.6341\n",
      "Epoch 26/300\n",
      "160/160 [==============================] - 0s 712us/sample - loss: 0.2354 - accuracy: 0.9563 - val_loss: 0.6872 - val_accuracy: 0.6829\n",
      "Epoch 27/300\n",
      "160/160 [==============================] - 0s 775us/sample - loss: 0.1808 - accuracy: 0.9812 - val_loss: 0.6516 - val_accuracy: 0.7317\n",
      "Epoch 28/300\n",
      "160/160 [==============================] - 0s 759us/sample - loss: 0.1532 - accuracy: 0.9750 - val_loss: 0.6297 - val_accuracy: 0.8049\n",
      "Epoch 29/300\n",
      "160/160 [==============================] - 0s 817us/sample - loss: 0.1334 - accuracy: 0.9875 - val_loss: 0.6672 - val_accuracy: 0.7317\n",
      "Epoch 30/300\n",
      "160/160 [==============================] - 0s 854us/sample - loss: 0.0939 - accuracy: 0.9938 - val_loss: 0.6084 - val_accuracy: 0.8293\n",
      "Epoch 31/300\n",
      "160/160 [==============================] - 0s 795us/sample - loss: 0.0777 - accuracy: 0.9938 - val_loss: 0.6449 - val_accuracy: 0.7317\n",
      "Epoch 32/300\n",
      "160/160 [==============================] - 0s 815us/sample - loss: 0.0583 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 0.8049\n",
      "Epoch 33/300\n",
      "160/160 [==============================] - 0s 806us/sample - loss: 0.0502 - accuracy: 1.0000 - val_loss: 0.6157 - val_accuracy: 0.8293\n",
      "Epoch 34/300\n",
      "160/160 [==============================] - 0s 792us/sample - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.6597 - val_accuracy: 0.8049\n",
      "Epoch 35/300\n",
      "160/160 [==============================] - 0s 771us/sample - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.5949 - val_accuracy: 0.8780\n",
      "Epoch 36/300\n",
      "160/160 [==============================] - 0s 728us/sample - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.6551 - val_accuracy: 0.8293\n",
      "Epoch 37/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.6630 - val_accuracy: 0.8293\n",
      "Epoch 38/300\n",
      "160/160 [==============================] - 0s 756us/sample - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.6018 - val_accuracy: 0.8537\n",
      "Epoch 39/300\n",
      "160/160 [==============================] - 0s 799us/sample - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.6444 - val_accuracy: 0.8293\n",
      "Epoch 40/300\n",
      "160/160 [==============================] - 0s 767us/sample - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.6622 - val_accuracy: 0.8293\n",
      "Epoch 41/300\n",
      "160/160 [==============================] - 0s 802us/sample - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.6343 - val_accuracy: 0.8293\n",
      "Epoch 42/300\n",
      "160/160 [==============================] - 0s 735us/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 0.8537\n",
      "Epoch 43/300\n",
      "160/160 [==============================] - 0s 731us/sample - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.6398 - val_accuracy: 0.8049\n",
      "Epoch 44/300\n",
      "160/160 [==============================] - 0s 727us/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6579 - val_accuracy: 0.7805\n",
      "Epoch 45/300\n",
      "160/160 [==============================] - 0s 828us/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.6403 - val_accuracy: 0.8780\n",
      "Epoch 46/300\n",
      "160/160 [==============================] - 0s 839us/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.6531 - val_accuracy: 0.8537\n",
      "Epoch 47/300\n",
      "160/160 [==============================] - 0s 816us/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.6862 - val_accuracy: 0.8293\n",
      "Epoch 48/300\n",
      "160/160 [==============================] - 0s 850us/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.6793 - val_accuracy: 0.8537\n",
      "Epoch 49/300\n",
      "160/160 [==============================] - 0s 860us/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.6443 - val_accuracy: 0.8537\n",
      "Epoch 50/300\n",
      "160/160 [==============================] - 0s 772us/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.6440 - val_accuracy: 0.8537\n",
      "Epoch 51/300\n",
      "160/160 [==============================] - 0s 803us/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.6608 - val_accuracy: 0.8537\n",
      "Epoch 52/300\n",
      "160/160 [==============================] - 0s 737us/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.6747 - val_accuracy: 0.8537\n",
      "Epoch 53/300\n",
      "160/160 [==============================] - 0s 783us/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.6661 - val_accuracy: 0.8537\n",
      "Epoch 54/300\n",
      "160/160 [==============================] - 0s 750us/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.6555 - val_accuracy: 0.8780\n",
      "Epoch 55/300\n",
      "160/160 [==============================] - 0s 830us/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.6584 - val_accuracy: 0.8780\n",
      "Epoch 56/300\n",
      "160/160 [==============================] - 0s 585us/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.6825 - val_accuracy: 0.8537\n",
      "Epoch 57/300\n",
      "160/160 [==============================] - 0s 665us/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.6878 - val_accuracy: 0.8537\n",
      "Epoch 58/300\n",
      "160/160 [==============================] - 0s 891us/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.6694 - val_accuracy: 0.8780\n",
      "Epoch 59/300\n",
      "160/160 [==============================] - 0s 721us/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.6613 - val_accuracy: 0.8780\n",
      "Epoch 60/300\n",
      "160/160 [==============================] - 0s 771us/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.6636 - val_accuracy: 0.8780\n",
      "Epoch 61/300\n",
      "160/160 [==============================] - 0s 814us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6820 - val_accuracy: 0.8537\n",
      "Epoch 62/300\n",
      "160/160 [==============================] - 0s 817us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6949 - val_accuracy: 0.8537\n",
      "Epoch 63/300\n",
      "160/160 [==============================] - 0s 742us/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6839 - val_accuracy: 0.8780\n",
      "Epoch 64/300\n",
      "160/160 [==============================] - 0s 720us/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6729 - val_accuracy: 0.8780\n",
      "Epoch 65/300\n",
      "160/160 [==============================] - 0s 687us/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6744 - val_accuracy: 0.8780\n",
      "Epoch 66/300\n",
      "160/160 [==============================] - 0s 654us/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6903 - val_accuracy: 0.8537\n",
      "Epoch 67/300\n",
      "160/160 [==============================] - 0s 628us/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6938 - val_accuracy: 0.8537\n",
      "Epoch 68/300\n",
      "160/160 [==============================] - 0s 695us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6928 - val_accuracy: 0.8537\n",
      "Epoch 69/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.8780\n",
      "Epoch 70/300\n",
      "160/160 [==============================] - 0s 807us/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.8780\n",
      "Epoch 71/300\n",
      "160/160 [==============================] - 0s 761us/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6940 - val_accuracy: 0.8537\n",
      "Epoch 72/300\n",
      "160/160 [==============================] - 0s 696us/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6949 - val_accuracy: 0.8537\n",
      "Epoch 73/300\n",
      "160/160 [==============================] - 0s 778us/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6910 - val_accuracy: 0.8537\n",
      "Epoch 74/300\n",
      "160/160 [==============================] - 0s 715us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6912 - val_accuracy: 0.8537\n",
      "Epoch 75/300\n",
      "160/160 [==============================] - 0s 771us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6937 - val_accuracy: 0.8537\n",
      "Epoch 76/300\n",
      "160/160 [==============================] - 0s 758us/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6974 - val_accuracy: 0.8537\n",
      "Epoch 77/300\n",
      "160/160 [==============================] - 0s 703us/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7047 - val_accuracy: 0.8537\n",
      "Epoch 78/300\n",
      "160/160 [==============================] - 0s 804us/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7024 - val_accuracy: 0.8537\n",
      "Epoch 79/300\n",
      "160/160 [==============================] - 0s 676us/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7017 - val_accuracy: 0.8537\n",
      "Epoch 80/300\n",
      "160/160 [==============================] - 0s 659us/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7034 - val_accuracy: 0.8537\n",
      "Epoch 81/300\n",
      "160/160 [==============================] - 0s 619us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7063 - val_accuracy: 0.8537\n",
      "Epoch 82/300\n",
      "160/160 [==============================] - 0s 834us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7153 - val_accuracy: 0.8537\n",
      "Epoch 83/300\n",
      "160/160 [==============================] - 0s 872us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7183 - val_accuracy: 0.8537\n",
      "Epoch 84/300\n",
      "160/160 [==============================] - 0s 794us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7151 - val_accuracy: 0.8537\n",
      "Epoch 85/300\n",
      "160/160 [==============================] - 0s 711us/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.7100 - val_accuracy: 0.8537\n",
      "Epoch 86/300\n",
      "160/160 [==============================] - 0s 749us/sample - loss: 9.9053e-04 - accuracy: 1.0000 - val_loss: 0.7093 - val_accuracy: 0.8537\n",
      "Epoch 87/300\n",
      "160/160 [==============================] - 0s 774us/sample - loss: 9.5919e-04 - accuracy: 1.0000 - val_loss: 0.7137 - val_accuracy: 0.8537\n",
      "Epoch 88/300\n",
      "160/160 [==============================] - 0s 755us/sample - loss: 9.3093e-04 - accuracy: 1.0000 - val_loss: 0.7180 - val_accuracy: 0.8537\n",
      "Epoch 89/300\n",
      "160/160 [==============================] - 0s 924us/sample - loss: 9.0939e-04 - accuracy: 1.0000 - val_loss: 0.7208 - val_accuracy: 0.8537\n",
      "Epoch 90/300\n",
      "160/160 [==============================] - 0s 676us/sample - loss: 8.8512e-04 - accuracy: 1.0000 - val_loss: 0.7177 - val_accuracy: 0.8537\n",
      "Epoch 91/300\n",
      "160/160 [==============================] - 0s 631us/sample - loss: 8.5962e-04 - accuracy: 1.0000 - val_loss: 0.7170 - val_accuracy: 0.8537\n",
      "Epoch 92/300\n",
      "160/160 [==============================] - 0s 715us/sample - loss: 8.3729e-04 - accuracy: 1.0000 - val_loss: 0.7178 - val_accuracy: 0.8537\n",
      "Epoch 93/300\n",
      "160/160 [==============================] - 0s 737us/sample - loss: 8.1404e-04 - accuracy: 1.0000 - val_loss: 0.7190 - val_accuracy: 0.8537\n",
      "Epoch 94/300\n",
      "160/160 [==============================] - 0s 782us/sample - loss: 7.9592e-04 - accuracy: 1.0000 - val_loss: 0.7217 - val_accuracy: 0.8537\n",
      "Epoch 95/300\n",
      "160/160 [==============================] - 0s 821us/sample - loss: 7.7902e-04 - accuracy: 1.0000 - val_loss: 0.7226 - val_accuracy: 0.8537\n",
      "Epoch 96/300\n",
      "160/160 [==============================] - 0s 764us/sample - loss: 7.5887e-04 - accuracy: 1.0000 - val_loss: 0.7227 - val_accuracy: 0.8780\n",
      "Epoch 97/300\n",
      "160/160 [==============================] - 0s 700us/sample - loss: 7.3877e-04 - accuracy: 1.0000 - val_loss: 0.7265 - val_accuracy: 0.8780\n",
      "Epoch 98/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 7.2107e-04 - accuracy: 1.0000 - val_loss: 0.7285 - val_accuracy: 0.8537\n",
      "Epoch 99/300\n",
      "160/160 [==============================] - 0s 711us/sample - loss: 6.9876e-04 - accuracy: 1.0000 - val_loss: 0.7305 - val_accuracy: 0.8537\n",
      "Epoch 100/300\n",
      "160/160 [==============================] - 0s 698us/sample - loss: 6.8257e-04 - accuracy: 1.0000 - val_loss: 0.7295 - val_accuracy: 0.8537\n",
      "Epoch 101/300\n",
      "160/160 [==============================] - 0s 838us/sample - loss: 6.6695e-04 - accuracy: 1.0000 - val_loss: 0.7297 - val_accuracy: 0.8537\n",
      "Epoch 102/300\n",
      "160/160 [==============================] - 0s 721us/sample - loss: 6.5186e-04 - accuracy: 1.0000 - val_loss: 0.7306 - val_accuracy: 0.8537\n",
      "Epoch 103/300\n",
      "160/160 [==============================] - 0s 745us/sample - loss: 6.3418e-04 - accuracy: 1.0000 - val_loss: 0.7303 - val_accuracy: 0.8537\n",
      "Epoch 104/300\n",
      "160/160 [==============================] - 0s 723us/sample - loss: 6.2384e-04 - accuracy: 1.0000 - val_loss: 0.7322 - val_accuracy: 0.8780\n",
      "Epoch 105/300\n",
      "160/160 [==============================] - 0s 675us/sample - loss: 6.1149e-04 - accuracy: 1.0000 - val_loss: 0.7349 - val_accuracy: 0.8537\n",
      "Epoch 106/300\n",
      "160/160 [==============================] - 0s 700us/sample - loss: 5.9452e-04 - accuracy: 1.0000 - val_loss: 0.7367 - val_accuracy: 0.8537\n",
      "Epoch 107/300\n",
      "160/160 [==============================] - 0s 691us/sample - loss: 5.8298e-04 - accuracy: 1.0000 - val_loss: 0.7388 - val_accuracy: 0.8537\n",
      "Epoch 108/300\n",
      "160/160 [==============================] - 0s 742us/sample - loss: 5.6630e-04 - accuracy: 1.0000 - val_loss: 0.7405 - val_accuracy: 0.8537\n",
      "Epoch 109/300\n",
      "160/160 [==============================] - 0s 836us/sample - loss: 5.5598e-04 - accuracy: 1.0000 - val_loss: 0.7424 - val_accuracy: 0.8537\n",
      "Epoch 110/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 5.4240e-04 - accuracy: 1.0000 - val_loss: 0.7407 - val_accuracy: 0.8537\n",
      "Epoch 111/300\n",
      "160/160 [==============================] - 0s 651us/sample - loss: 5.2983e-04 - accuracy: 1.0000 - val_loss: 0.7401 - val_accuracy: 0.8537\n",
      "Epoch 112/300\n",
      "160/160 [==============================] - 0s 869us/sample - loss: 5.2251e-04 - accuracy: 1.0000 - val_loss: 0.7379 - val_accuracy: 0.8780\n",
      "Epoch 113/300\n",
      "160/160 [==============================] - 0s 853us/sample - loss: 5.1263e-04 - accuracy: 1.0000 - val_loss: 0.7414 - val_accuracy: 0.8537\n",
      "Epoch 114/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 4.9965e-04 - accuracy: 1.0000 - val_loss: 0.7453 - val_accuracy: 0.8537\n",
      "Epoch 115/300\n",
      "160/160 [==============================] - 0s 793us/sample - loss: 4.8730e-04 - accuracy: 1.0000 - val_loss: 0.7463 - val_accuracy: 0.8537\n",
      "Epoch 116/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 4.8042e-04 - accuracy: 1.0000 - val_loss: 0.7486 - val_accuracy: 0.8537\n",
      "Epoch 117/300\n",
      "160/160 [==============================] - 0s 829us/sample - loss: 4.6860e-04 - accuracy: 1.0000 - val_loss: 0.7489 - val_accuracy: 0.8537\n",
      "Epoch 118/300\n",
      "160/160 [==============================] - 0s 936us/sample - loss: 4.5870e-04 - accuracy: 1.0000 - val_loss: 0.7492 - val_accuracy: 0.8537\n",
      "Epoch 119/300\n",
      "160/160 [==============================] - 0s 852us/sample - loss: 4.4913e-04 - accuracy: 1.0000 - val_loss: 0.7488 - val_accuracy: 0.8537\n",
      "Epoch 120/300\n",
      "160/160 [==============================] - 0s 864us/sample - loss: 4.4188e-04 - accuracy: 1.0000 - val_loss: 0.7482 - val_accuracy: 0.8537\n",
      "Epoch 121/300\n",
      "160/160 [==============================] - 0s 855us/sample - loss: 4.3183e-04 - accuracy: 1.0000 - val_loss: 0.7513 - val_accuracy: 0.8537\n",
      "Epoch 122/300\n",
      "160/160 [==============================] - 0s 855us/sample - loss: 4.2532e-04 - accuracy: 1.0000 - val_loss: 0.7559 - val_accuracy: 0.8537\n",
      "Epoch 123/300\n",
      "160/160 [==============================] - 0s 765us/sample - loss: 4.1626e-04 - accuracy: 1.0000 - val_loss: 0.7563 - val_accuracy: 0.8537\n",
      "Epoch 124/300\n",
      "160/160 [==============================] - 0s 775us/sample - loss: 4.0627e-04 - accuracy: 1.0000 - val_loss: 0.7545 - val_accuracy: 0.8537\n",
      "Epoch 125/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 3.9802e-04 - accuracy: 1.0000 - val_loss: 0.7548 - val_accuracy: 0.8537\n",
      "Epoch 126/300\n",
      "160/160 [==============================] - 0s 801us/sample - loss: 3.9346e-04 - accuracy: 1.0000 - val_loss: 0.7566 - val_accuracy: 0.8780\n",
      "Epoch 127/300\n",
      "160/160 [==============================] - 0s 698us/sample - loss: 3.8488e-04 - accuracy: 1.0000 - val_loss: 0.7600 - val_accuracy: 0.8537\n",
      "Epoch 128/300\n",
      "160/160 [==============================] - 0s 759us/sample - loss: 3.7665e-04 - accuracy: 1.0000 - val_loss: 0.7632 - val_accuracy: 0.8537\n",
      "Epoch 129/300\n",
      "160/160 [==============================] - 0s 831us/sample - loss: 3.7012e-04 - accuracy: 1.0000 - val_loss: 0.7656 - val_accuracy: 0.8537\n",
      "Epoch 130/300\n",
      "160/160 [==============================] - 0s 791us/sample - loss: 3.6432e-04 - accuracy: 1.0000 - val_loss: 0.7669 - val_accuracy: 0.8537\n",
      "Epoch 131/300\n",
      "160/160 [==============================] - 0s 826us/sample - loss: 3.5698e-04 - accuracy: 1.0000 - val_loss: 0.7675 - val_accuracy: 0.8537\n",
      "Epoch 132/300\n",
      "160/160 [==============================] - 0s 981us/sample - loss: 3.4896e-04 - accuracy: 1.0000 - val_loss: 0.7660 - val_accuracy: 0.8537\n",
      "Epoch 133/300\n",
      "160/160 [==============================] - 0s 826us/sample - loss: 3.4332e-04 - accuracy: 1.0000 - val_loss: 0.7647 - val_accuracy: 0.8537\n",
      "Epoch 134/300\n",
      "160/160 [==============================] - 0s 784us/sample - loss: 3.3834e-04 - accuracy: 1.0000 - val_loss: 0.7641 - val_accuracy: 0.8537\n",
      "Epoch 135/300\n",
      "160/160 [==============================] - 0s 765us/sample - loss: 3.3208e-04 - accuracy: 1.0000 - val_loss: 0.7655 - val_accuracy: 0.8537\n",
      "Epoch 136/300\n",
      "160/160 [==============================] - 0s 791us/sample - loss: 3.2600e-04 - accuracy: 1.0000 - val_loss: 0.7686 - val_accuracy: 0.8537\n",
      "Epoch 137/300\n",
      "160/160 [==============================] - 0s 790us/sample - loss: 3.1968e-04 - accuracy: 1.0000 - val_loss: 0.7724 - val_accuracy: 0.8537\n",
      "Epoch 138/300\n",
      "160/160 [==============================] - 0s 783us/sample - loss: 3.1413e-04 - accuracy: 1.0000 - val_loss: 0.7753 - val_accuracy: 0.8537\n",
      "Epoch 139/300\n",
      "160/160 [==============================] - 0s 718us/sample - loss: 3.1051e-04 - accuracy: 1.0000 - val_loss: 0.7773 - val_accuracy: 0.8537\n",
      "Epoch 140/300\n",
      "160/160 [==============================] - 0s 659us/sample - loss: 3.0409e-04 - accuracy: 1.0000 - val_loss: 0.7767 - val_accuracy: 0.8780\n",
      "Epoch 141/300\n",
      "160/160 [==============================] - 0s 664us/sample - loss: 2.9966e-04 - accuracy: 1.0000 - val_loss: 0.7770 - val_accuracy: 0.8780\n",
      "Epoch 142/300\n",
      "160/160 [==============================] - 0s 693us/sample - loss: 2.9364e-04 - accuracy: 1.0000 - val_loss: 0.7762 - val_accuracy: 0.8537\n",
      "Epoch 143/300\n",
      "160/160 [==============================] - 0s 928us/sample - loss: 2.8776e-04 - accuracy: 1.0000 - val_loss: 0.7768 - val_accuracy: 0.8537\n",
      "Epoch 144/300\n",
      "160/160 [==============================] - 0s 836us/sample - loss: 2.8341e-04 - accuracy: 1.0000 - val_loss: 0.7783 - val_accuracy: 0.8537\n",
      "Epoch 145/300\n",
      "160/160 [==============================] - 0s 763us/sample - loss: 2.8098e-04 - accuracy: 1.0000 - val_loss: 0.7788 - val_accuracy: 0.8537\n",
      "Epoch 146/300\n",
      "160/160 [==============================] - 0s 791us/sample - loss: 2.7773e-04 - accuracy: 1.0000 - val_loss: 0.7778 - val_accuracy: 0.8537\n",
      "Epoch 147/300\n",
      "160/160 [==============================] - 0s 798us/sample - loss: 2.7178e-04 - accuracy: 1.0000 - val_loss: 0.7770 - val_accuracy: 0.8537\n",
      "Epoch 148/300\n",
      "160/160 [==============================] - 0s 800us/sample - loss: 2.6766e-04 - accuracy: 1.0000 - val_loss: 0.7777 - val_accuracy: 0.8537\n",
      "Epoch 149/300\n",
      "160/160 [==============================] - 0s 755us/sample - loss: 2.6196e-04 - accuracy: 1.0000 - val_loss: 0.7810 - val_accuracy: 0.8537\n",
      "Epoch 150/300\n",
      "160/160 [==============================] - 0s 763us/sample - loss: 2.5736e-04 - accuracy: 1.0000 - val_loss: 0.7847 - val_accuracy: 0.8537\n",
      "Epoch 151/300\n",
      "160/160 [==============================] - 0s 723us/sample - loss: 2.5345e-04 - accuracy: 1.0000 - val_loss: 0.7873 - val_accuracy: 0.8537\n",
      "Epoch 152/300\n",
      "160/160 [==============================] - 0s 927us/sample - loss: 2.4964e-04 - accuracy: 1.0000 - val_loss: 0.7891 - val_accuracy: 0.8537\n",
      "Epoch 153/300\n",
      "160/160 [==============================] - 0s 825us/sample - loss: 2.4466e-04 - accuracy: 1.0000 - val_loss: 0.7894 - val_accuracy: 0.8780\n",
      "Epoch 154/300\n",
      "160/160 [==============================] - 0s 779us/sample - loss: 2.4218e-04 - accuracy: 1.0000 - val_loss: 0.7891 - val_accuracy: 0.8780\n",
      "Epoch 155/300\n",
      "160/160 [==============================] - 0s 763us/sample - loss: 2.3885e-04 - accuracy: 1.0000 - val_loss: 0.7911 - val_accuracy: 0.8780\n",
      "Epoch 156/300\n",
      "160/160 [==============================] - 0s 749us/sample - loss: 2.3457e-04 - accuracy: 1.0000 - val_loss: 0.7931 - val_accuracy: 0.8780\n",
      "Epoch 157/300\n",
      "160/160 [==============================] - 0s 796us/sample - loss: 2.3086e-04 - accuracy: 1.0000 - val_loss: 0.7962 - val_accuracy: 0.8537\n",
      "Epoch 158/300\n",
      "160/160 [==============================] - 0s 754us/sample - loss: 2.2644e-04 - accuracy: 1.0000 - val_loss: 0.7989 - val_accuracy: 0.8537\n",
      "Epoch 159/300\n",
      "160/160 [==============================] - 0s 792us/sample - loss: 2.2359e-04 - accuracy: 1.0000 - val_loss: 0.8006 - val_accuracy: 0.8537\n",
      "Epoch 160/300\n",
      "160/160 [==============================] - 0s 794us/sample - loss: 2.2018e-04 - accuracy: 1.0000 - val_loss: 0.7999 - val_accuracy: 0.8537\n",
      "Epoch 161/300\n",
      "160/160 [==============================] - 0s 765us/sample - loss: 2.1699e-04 - accuracy: 1.0000 - val_loss: 0.7984 - val_accuracy: 0.8780\n",
      "Epoch 162/300\n",
      "160/160 [==============================] - 0s 762us/sample - loss: 2.1325e-04 - accuracy: 1.0000 - val_loss: 0.7987 - val_accuracy: 0.8537\n",
      "Epoch 163/300\n",
      "160/160 [==============================] - 0s 835us/sample - loss: 2.0974e-04 - accuracy: 1.0000 - val_loss: 0.7990 - val_accuracy: 0.8537\n",
      "Epoch 164/300\n",
      "160/160 [==============================] - 0s 836us/sample - loss: 2.0691e-04 - accuracy: 1.0000 - val_loss: 0.7993 - val_accuracy: 0.8537\n",
      "Epoch 165/300\n",
      "160/160 [==============================] - 0s 847us/sample - loss: 2.0385e-04 - accuracy: 1.0000 - val_loss: 0.7996 - val_accuracy: 0.8537\n",
      "Epoch 166/300\n",
      "160/160 [==============================] - 0s 746us/sample - loss: 2.0145e-04 - accuracy: 1.0000 - val_loss: 0.8014 - val_accuracy: 0.8537\n",
      "Epoch 167/300\n",
      "160/160 [==============================] - 0s 688us/sample - loss: 1.9784e-04 - accuracy: 1.0000 - val_loss: 0.8039 - val_accuracy: 0.8537\n",
      "Epoch 168/300\n",
      "160/160 [==============================] - 0s 684us/sample - loss: 1.9499e-04 - accuracy: 1.0000 - val_loss: 0.8053 - val_accuracy: 0.8537\n",
      "Epoch 169/300\n",
      "160/160 [==============================] - 0s 673us/sample - loss: 1.9231e-04 - accuracy: 1.0000 - val_loss: 0.8052 - val_accuracy: 0.8537\n",
      "Epoch 170/300\n",
      "160/160 [==============================] - 0s 713us/sample - loss: 1.8964e-04 - accuracy: 1.0000 - val_loss: 0.8056 - val_accuracy: 0.8537\n",
      "Epoch 171/300\n",
      "160/160 [==============================] - 0s 807us/sample - loss: 1.8725e-04 - accuracy: 1.0000 - val_loss: 0.8063 - val_accuracy: 0.8537\n",
      "Epoch 172/300\n",
      "160/160 [==============================] - 0s 889us/sample - loss: 1.8461e-04 - accuracy: 1.0000 - val_loss: 0.8061 - val_accuracy: 0.8537\n",
      "Epoch 173/300\n",
      "160/160 [==============================] - 0s 805us/sample - loss: 1.8217e-04 - accuracy: 1.0000 - val_loss: 0.8067 - val_accuracy: 0.8537\n",
      "Epoch 174/300\n",
      "160/160 [==============================] - 0s 750us/sample - loss: 1.7914e-04 - accuracy: 1.0000 - val_loss: 0.8081 - val_accuracy: 0.8537\n",
      "Epoch 175/300\n",
      "160/160 [==============================] - 0s 759us/sample - loss: 1.7659e-04 - accuracy: 1.0000 - val_loss: 0.8095 - val_accuracy: 0.8537\n",
      "Epoch 176/300\n",
      "160/160 [==============================] - 0s 764us/sample - loss: 1.7447e-04 - accuracy: 1.0000 - val_loss: 0.8118 - val_accuracy: 0.8537\n",
      "Epoch 177/300\n",
      "160/160 [==============================] - 0s 773us/sample - loss: 1.7305e-04 - accuracy: 1.0000 - val_loss: 0.8135 - val_accuracy: 0.8780\n",
      "Epoch 178/300\n",
      "160/160 [==============================] - 0s 759us/sample - loss: 1.7057e-04 - accuracy: 1.0000 - val_loss: 0.8144 - val_accuracy: 0.8780\n",
      "Epoch 179/300\n",
      "160/160 [==============================] - 0s 723us/sample - loss: 1.6756e-04 - accuracy: 1.0000 - val_loss: 0.8155 - val_accuracy: 0.8780\n",
      "Epoch 180/300\n",
      "160/160 [==============================] - 0s 716us/sample - loss: 1.6546e-04 - accuracy: 1.0000 - val_loss: 0.8158 - val_accuracy: 0.8780\n",
      "Epoch 181/300\n",
      "160/160 [==============================] - 0s 819us/sample - loss: 1.6311e-04 - accuracy: 1.0000 - val_loss: 0.8172 - val_accuracy: 0.8537\n",
      "Epoch 182/300\n",
      "160/160 [==============================] - 0s 840us/sample - loss: 1.6098e-04 - accuracy: 1.0000 - val_loss: 0.8180 - val_accuracy: 0.8537\n",
      "Epoch 183/300\n",
      "160/160 [==============================] - 0s 725us/sample - loss: 1.5887e-04 - accuracy: 1.0000 - val_loss: 0.8181 - val_accuracy: 0.8537\n",
      "Epoch 184/300\n",
      "160/160 [==============================] - 0s 711us/sample - loss: 1.5732e-04 - accuracy: 1.0000 - val_loss: 0.8172 - val_accuracy: 0.8537\n",
      "Epoch 185/300\n",
      "160/160 [==============================] - 0s 701us/sample - loss: 1.5501e-04 - accuracy: 1.0000 - val_loss: 0.8171 - val_accuracy: 0.8537\n",
      "Epoch 186/300\n",
      "160/160 [==============================] - 0s 728us/sample - loss: 1.5278e-04 - accuracy: 1.0000 - val_loss: 0.8163 - val_accuracy: 0.8537\n",
      "Epoch 187/300\n",
      "160/160 [==============================] - 0s 726us/sample - loss: 1.5099e-04 - accuracy: 1.0000 - val_loss: 0.8175 - val_accuracy: 0.8780\n",
      "Epoch 188/300\n",
      "160/160 [==============================] - 0s 693us/sample - loss: 1.4881e-04 - accuracy: 1.0000 - val_loss: 0.8185 - val_accuracy: 0.8780\n",
      "Epoch 189/300\n",
      "160/160 [==============================] - 0s 761us/sample - loss: 1.4781e-04 - accuracy: 1.0000 - val_loss: 0.8210 - val_accuracy: 0.8780\n",
      "Epoch 190/300\n",
      "160/160 [==============================] - 0s 761us/sample - loss: 1.4566e-04 - accuracy: 1.0000 - val_loss: 0.8226 - val_accuracy: 0.8780\n",
      "Epoch 191/300\n",
      "160/160 [==============================] - 0s 701us/sample - loss: 1.4405e-04 - accuracy: 1.0000 - val_loss: 0.8249 - val_accuracy: 0.8780\n",
      "Epoch 192/300\n",
      "160/160 [==============================] - 0s 718us/sample - loss: 1.4165e-04 - accuracy: 1.0000 - val_loss: 0.8274 - val_accuracy: 0.8537\n",
      "Epoch 193/300\n",
      "160/160 [==============================] - 0s 996us/sample - loss: 1.4002e-04 - accuracy: 1.0000 - val_loss: 0.8290 - val_accuracy: 0.8537\n",
      "Epoch 194/300\n",
      "160/160 [==============================] - 0s 710us/sample - loss: 1.3811e-04 - accuracy: 1.0000 - val_loss: 0.8297 - val_accuracy: 0.8537\n",
      "Epoch 195/300\n",
      "160/160 [==============================] - 0s 661us/sample - loss: 1.3630e-04 - accuracy: 1.0000 - val_loss: 0.8308 - val_accuracy: 0.8537\n",
      "Epoch 196/300\n",
      "160/160 [==============================] - 0s 707us/sample - loss: 1.3473e-04 - accuracy: 1.0000 - val_loss: 0.8320 - val_accuracy: 0.8537\n",
      "Epoch 197/300\n",
      "160/160 [==============================] - 0s 706us/sample - loss: 1.3274e-04 - accuracy: 1.0000 - val_loss: 0.8315 - val_accuracy: 0.8537\n",
      "Epoch 198/300\n",
      "160/160 [==============================] - 0s 734us/sample - loss: 1.3084e-04 - accuracy: 1.0000 - val_loss: 0.8312 - val_accuracy: 0.8780\n",
      "Epoch 199/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 1.2962e-04 - accuracy: 1.0000 - val_loss: 0.8308 - val_accuracy: 0.8780\n",
      "Epoch 200/300\n",
      "160/160 [==============================] - 0s 730us/sample - loss: 1.2843e-04 - accuracy: 1.0000 - val_loss: 0.8316 - val_accuracy: 0.8537\n",
      "Epoch 201/300\n",
      "160/160 [==============================] - 0s 768us/sample - loss: 1.2674e-04 - accuracy: 1.0000 - val_loss: 0.8328 - val_accuracy: 0.8537\n",
      "Epoch 202/300\n",
      "160/160 [==============================] - 0s 727us/sample - loss: 1.2511e-04 - accuracy: 1.0000 - val_loss: 0.8356 - val_accuracy: 0.8537\n",
      "Epoch 203/300\n",
      "160/160 [==============================] - 0s 750us/sample - loss: 1.2367e-04 - accuracy: 1.0000 - val_loss: 0.8382 - val_accuracy: 0.8537\n",
      "Epoch 204/300\n",
      "160/160 [==============================] - 0s 825us/sample - loss: 1.2216e-04 - accuracy: 1.0000 - val_loss: 0.8398 - val_accuracy: 0.8537\n",
      "Epoch 205/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 1.2058e-04 - accuracy: 1.0000 - val_loss: 0.8402 - val_accuracy: 0.8537\n",
      "Epoch 206/300\n",
      "160/160 [==============================] - 0s 712us/sample - loss: 1.1950e-04 - accuracy: 1.0000 - val_loss: 0.8401 - val_accuracy: 0.8780\n",
      "Epoch 207/300\n",
      "160/160 [==============================] - 0s 779us/sample - loss: 1.1813e-04 - accuracy: 1.0000 - val_loss: 0.8410 - val_accuracy: 0.8537\n",
      "Epoch 208/300\n",
      "160/160 [==============================] - 0s 749us/sample - loss: 1.1656e-04 - accuracy: 1.0000 - val_loss: 0.8428 - val_accuracy: 0.8537\n",
      "Epoch 209/300\n",
      "160/160 [==============================] - 0s 735us/sample - loss: 1.1516e-04 - accuracy: 1.0000 - val_loss: 0.8446 - val_accuracy: 0.8537\n",
      "Epoch 210/300\n",
      "160/160 [==============================] - 0s 693us/sample - loss: 1.1406e-04 - accuracy: 1.0000 - val_loss: 0.8467 - val_accuracy: 0.8537\n",
      "Epoch 211/300\n",
      "160/160 [==============================] - 0s 716us/sample - loss: 1.1259e-04 - accuracy: 1.0000 - val_loss: 0.8466 - val_accuracy: 0.8537\n",
      "Epoch 212/300\n",
      "160/160 [==============================] - 0s 723us/sample - loss: 1.1113e-04 - accuracy: 1.0000 - val_loss: 0.8467 - val_accuracy: 0.8537\n",
      "Epoch 213/300\n",
      "160/160 [==============================] - 0s 807us/sample - loss: 1.0989e-04 - accuracy: 1.0000 - val_loss: 0.8460 - val_accuracy: 0.8537\n",
      "Epoch 214/300\n",
      "160/160 [==============================] - 0s 703us/sample - loss: 1.0866e-04 - accuracy: 1.0000 - val_loss: 0.8455 - val_accuracy: 0.8537\n",
      "Epoch 215/300\n",
      "160/160 [==============================] - 0s 689us/sample - loss: 1.0752e-04 - accuracy: 1.0000 - val_loss: 0.8457 - val_accuracy: 0.8537\n",
      "Epoch 216/300\n",
      "160/160 [==============================] - 0s 623us/sample - loss: 1.0626e-04 - accuracy: 1.0000 - val_loss: 0.8462 - val_accuracy: 0.8537\n",
      "Epoch 217/300\n",
      "160/160 [==============================] - 0s 674us/sample - loss: 1.0572e-04 - accuracy: 1.0000 - val_loss: 0.8473 - val_accuracy: 0.8537\n",
      "Epoch 218/300\n",
      "160/160 [==============================] - 0s 704us/sample - loss: 1.0449e-04 - accuracy: 1.0000 - val_loss: 0.8482 - val_accuracy: 0.8780\n",
      "Epoch 219/300\n",
      "160/160 [==============================] - 0s 727us/sample - loss: 1.0294e-04 - accuracy: 1.0000 - val_loss: 0.8502 - val_accuracy: 0.8780\n",
      "Epoch 220/300\n",
      "160/160 [==============================] - 0s 820us/sample - loss: 1.0181e-04 - accuracy: 1.0000 - val_loss: 0.8523 - val_accuracy: 0.8780\n",
      "Epoch 221/300\n",
      "160/160 [==============================] - 0s 725us/sample - loss: 1.0076e-04 - accuracy: 1.0000 - val_loss: 0.8547 - val_accuracy: 0.8780\n",
      "Epoch 222/300\n",
      "160/160 [==============================] - 0s 793us/sample - loss: 9.9893e-05 - accuracy: 1.0000 - val_loss: 0.8558 - val_accuracy: 0.8537\n",
      "Epoch 223/300\n",
      "160/160 [==============================] - 0s 647us/sample - loss: 9.8704e-05 - accuracy: 1.0000 - val_loss: 0.8559 - val_accuracy: 0.8780\n",
      "Epoch 224/300\n",
      "160/160 [==============================] - 0s 687us/sample - loss: 9.7366e-05 - accuracy: 1.0000 - val_loss: 0.8565 - val_accuracy: 0.8780\n",
      "Epoch 225/300\n",
      "160/160 [==============================] - 0s 710us/sample - loss: 9.6370e-05 - accuracy: 1.0000 - val_loss: 0.8561 - val_accuracy: 0.8780\n",
      "Epoch 226/300\n",
      "160/160 [==============================] - 0s 897us/sample - loss: 9.5206e-05 - accuracy: 1.0000 - val_loss: 0.8546 - val_accuracy: 0.8780\n",
      "Epoch 227/300\n",
      "160/160 [==============================] - 0s 789us/sample - loss: 9.4423e-05 - accuracy: 1.0000 - val_loss: 0.8537 - val_accuracy: 0.8780\n",
      "Epoch 228/300\n",
      "160/160 [==============================] - 0s 801us/sample - loss: 9.3681e-05 - accuracy: 1.0000 - val_loss: 0.8535 - val_accuracy: 0.8780\n",
      "Epoch 229/300\n",
      "160/160 [==============================] - 0s 786us/sample - loss: 9.2689e-05 - accuracy: 1.0000 - val_loss: 0.8546 - val_accuracy: 0.8780\n",
      "Epoch 230/300\n",
      "160/160 [==============================] - 0s 754us/sample - loss: 9.1709e-05 - accuracy: 1.0000 - val_loss: 0.8554 - val_accuracy: 0.8780\n",
      "Epoch 231/300\n",
      "160/160 [==============================] - 0s 825us/sample - loss: 9.0562e-05 - accuracy: 1.0000 - val_loss: 0.8579 - val_accuracy: 0.8537\n",
      "Epoch 232/300\n",
      "160/160 [==============================] - 0s 810us/sample - loss: 8.9622e-05 - accuracy: 1.0000 - val_loss: 0.8608 - val_accuracy: 0.8537\n",
      "Epoch 233/300\n",
      "160/160 [==============================] - 0s 814us/sample - loss: 8.8639e-05 - accuracy: 1.0000 - val_loss: 0.8633 - val_accuracy: 0.8537\n",
      "Epoch 234/300\n",
      "160/160 [==============================] - 0s 743us/sample - loss: 8.7844e-05 - accuracy: 1.0000 - val_loss: 0.8654 - val_accuracy: 0.8537\n",
      "Epoch 235/300\n",
      "160/160 [==============================] - 0s 702us/sample - loss: 8.6897e-05 - accuracy: 1.0000 - val_loss: 0.8656 - val_accuracy: 0.8537\n",
      "Epoch 236/300\n",
      "160/160 [==============================] - 0s 807us/sample - loss: 8.5881e-05 - accuracy: 1.0000 - val_loss: 0.8657 - val_accuracy: 0.8537\n",
      "Epoch 237/300\n",
      "160/160 [==============================] - 0s 774us/sample - loss: 8.4942e-05 - accuracy: 1.0000 - val_loss: 0.8659 - val_accuracy: 0.8537\n",
      "Epoch 238/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 8.4096e-05 - accuracy: 1.0000 - val_loss: 0.8658 - val_accuracy: 0.8537\n",
      "Epoch 239/300\n",
      "160/160 [==============================] - 0s 683us/sample - loss: 8.3355e-05 - accuracy: 1.0000 - val_loss: 0.8671 - val_accuracy: 0.8537\n",
      "Epoch 240/300\n",
      "160/160 [==============================] - 0s 676us/sample - loss: 8.2319e-05 - accuracy: 1.0000 - val_loss: 0.8669 - val_accuracy: 0.8780\n",
      "Epoch 241/300\n",
      "160/160 [==============================] - 0s 683us/sample - loss: 8.1607e-05 - accuracy: 1.0000 - val_loss: 0.8665 - val_accuracy: 0.8780\n",
      "Epoch 242/300\n",
      "160/160 [==============================] - 0s 679us/sample - loss: 8.0886e-05 - accuracy: 1.0000 - val_loss: 0.8672 - val_accuracy: 0.8780\n",
      "Epoch 243/300\n",
      "160/160 [==============================] - 0s 723us/sample - loss: 7.9980e-05 - accuracy: 1.0000 - val_loss: 0.8692 - val_accuracy: 0.8780\n",
      "Epoch 244/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 7.9126e-05 - accuracy: 1.0000 - val_loss: 0.8715 - val_accuracy: 0.8780\n",
      "Epoch 245/300\n",
      "160/160 [==============================] - 0s 761us/sample - loss: 7.8505e-05 - accuracy: 1.0000 - val_loss: 0.8734 - val_accuracy: 0.8537\n",
      "Epoch 246/300\n",
      "160/160 [==============================] - 0s 984us/sample - loss: 7.7720e-05 - accuracy: 1.0000 - val_loss: 0.8742 - val_accuracy: 0.8780\n",
      "Epoch 247/300\n",
      "160/160 [==============================] - 0s 741us/sample - loss: 7.6899e-05 - accuracy: 1.0000 - val_loss: 0.8747 - val_accuracy: 0.8537\n",
      "Epoch 248/300\n",
      "160/160 [==============================] - 0s 634us/sample - loss: 7.6276e-05 - accuracy: 1.0000 - val_loss: 0.8738 - val_accuracy: 0.8537\n",
      "Epoch 249/300\n",
      "160/160 [==============================] - 0s 674us/sample - loss: 7.5321e-05 - accuracy: 1.0000 - val_loss: 0.8741 - val_accuracy: 0.8780\n",
      "Epoch 250/300\n",
      "160/160 [==============================] - 0s 714us/sample - loss: 7.4715e-05 - accuracy: 1.0000 - val_loss: 0.8747 - val_accuracy: 0.8780\n",
      "Epoch 251/300\n",
      "160/160 [==============================] - 0s 738us/sample - loss: 7.3977e-05 - accuracy: 1.0000 - val_loss: 0.8760 - val_accuracy: 0.8780\n",
      "Epoch 252/300\n",
      "160/160 [==============================] - 0s 731us/sample - loss: 7.3225e-05 - accuracy: 1.0000 - val_loss: 0.8770 - val_accuracy: 0.8780\n",
      "Epoch 253/300\n",
      "160/160 [==============================] - 0s 744us/sample - loss: 7.2479e-05 - accuracy: 1.0000 - val_loss: 0.8785 - val_accuracy: 0.8780\n",
      "Epoch 254/300\n",
      "160/160 [==============================] - 0s 720us/sample - loss: 7.1792e-05 - accuracy: 1.0000 - val_loss: 0.8797 - val_accuracy: 0.8537\n",
      "Epoch 255/300\n",
      "160/160 [==============================] - 0s 683us/sample - loss: 7.1163e-05 - accuracy: 1.0000 - val_loss: 0.8807 - val_accuracy: 0.8537\n",
      "Epoch 256/300\n",
      "160/160 [==============================] - 0s 639us/sample - loss: 7.0520e-05 - accuracy: 1.0000 - val_loss: 0.8819 - val_accuracy: 0.8537\n",
      "Epoch 257/300\n",
      "160/160 [==============================] - 0s 668us/sample - loss: 6.9890e-05 - accuracy: 1.0000 - val_loss: 0.8826 - val_accuracy: 0.8537\n",
      "Epoch 258/300\n",
      "160/160 [==============================] - 0s 700us/sample - loss: 6.9210e-05 - accuracy: 1.0000 - val_loss: 0.8827 - val_accuracy: 0.8537\n",
      "Epoch 259/300\n",
      "160/160 [==============================] - 0s 710us/sample - loss: 6.8593e-05 - accuracy: 1.0000 - val_loss: 0.8832 - val_accuracy: 0.8537\n",
      "Epoch 260/300\n",
      "160/160 [==============================] - 0s 730us/sample - loss: 6.8015e-05 - accuracy: 1.0000 - val_loss: 0.8832 - val_accuracy: 0.8537\n",
      "Epoch 261/300\n",
      "160/160 [==============================] - 0s 738us/sample - loss: 6.7394e-05 - accuracy: 1.0000 - val_loss: 0.8827 - val_accuracy: 0.8537\n",
      "Epoch 262/300\n",
      "160/160 [==============================] - 0s 713us/sample - loss: 6.6876e-05 - accuracy: 1.0000 - val_loss: 0.8826 - val_accuracy: 0.8537\n",
      "Epoch 263/300\n",
      "160/160 [==============================] - 0s 715us/sample - loss: 6.6355e-05 - accuracy: 1.0000 - val_loss: 0.8828 - val_accuracy: 0.8537\n",
      "Epoch 264/300\n",
      "160/160 [==============================] - 0s 779us/sample - loss: 6.5641e-05 - accuracy: 1.0000 - val_loss: 0.8833 - val_accuracy: 0.8780\n",
      "Epoch 265/300\n",
      "160/160 [==============================] - 0s 705us/sample - loss: 6.5045e-05 - accuracy: 1.0000 - val_loss: 0.8850 - val_accuracy: 0.8780\n",
      "Epoch 266/300\n",
      "160/160 [==============================] - 0s 737us/sample - loss: 6.4420e-05 - accuracy: 1.0000 - val_loss: 0.8872 - val_accuracy: 0.8537\n",
      "Epoch 267/300\n",
      "160/160 [==============================] - 0s 701us/sample - loss: 6.3989e-05 - accuracy: 1.0000 - val_loss: 0.8896 - val_accuracy: 0.8537\n",
      "Epoch 268/300\n",
      "160/160 [==============================] - 0s 740us/sample - loss: 6.3210e-05 - accuracy: 1.0000 - val_loss: 0.8907 - val_accuracy: 0.8537\n",
      "Epoch 269/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 6.2702e-05 - accuracy: 1.0000 - val_loss: 0.8911 - val_accuracy: 0.8537\n",
      "Epoch 270/300\n",
      "160/160 [==============================] - 0s 786us/sample - loss: 6.2109e-05 - accuracy: 1.0000 - val_loss: 0.8921 - val_accuracy: 0.8537\n",
      "Epoch 271/300\n",
      "160/160 [==============================] - 0s 708us/sample - loss: 6.1649e-05 - accuracy: 1.0000 - val_loss: 0.8918 - val_accuracy: 0.8537\n",
      "Epoch 272/300\n",
      "160/160 [==============================] - 0s 692us/sample - loss: 6.1023e-05 - accuracy: 1.0000 - val_loss: 0.8926 - val_accuracy: 0.8537\n",
      "Epoch 273/300\n",
      "160/160 [==============================] - 0s 633us/sample - loss: 6.0443e-05 - accuracy: 1.0000 - val_loss: 0.8939 - val_accuracy: 0.8537\n",
      "Epoch 274/300\n",
      "160/160 [==============================] - 0s 690us/sample - loss: 5.9933e-05 - accuracy: 1.0000 - val_loss: 0.8949 - val_accuracy: 0.8537\n",
      "Epoch 275/300\n",
      "160/160 [==============================] - 0s 774us/sample - loss: 5.9443e-05 - accuracy: 1.0000 - val_loss: 0.8948 - val_accuracy: 0.8537\n",
      "Epoch 276/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 5.8895e-05 - accuracy: 1.0000 - val_loss: 0.8948 - val_accuracy: 0.8537\n",
      "Epoch 277/300\n",
      "160/160 [==============================] - 0s 793us/sample - loss: 5.8405e-05 - accuracy: 1.0000 - val_loss: 0.8955 - val_accuracy: 0.8537\n",
      "Epoch 278/300\n",
      "160/160 [==============================] - 0s 824us/sample - loss: 5.7875e-05 - accuracy: 1.0000 - val_loss: 0.8961 - val_accuracy: 0.8537\n",
      "Epoch 279/300\n",
      "160/160 [==============================] - 0s 793us/sample - loss: 5.7338e-05 - accuracy: 1.0000 - val_loss: 0.8963 - val_accuracy: 0.8537\n",
      "Epoch 280/300\n",
      "160/160 [==============================] - 0s 698us/sample - loss: 5.6831e-05 - accuracy: 1.0000 - val_loss: 0.8966 - val_accuracy: 0.8537\n",
      "Epoch 281/300\n",
      "160/160 [==============================] - 0s 779us/sample - loss: 5.6388e-05 - accuracy: 1.0000 - val_loss: 0.8972 - val_accuracy: 0.8537\n",
      "Epoch 282/300\n",
      "160/160 [==============================] - 0s 734us/sample - loss: 5.5936e-05 - accuracy: 1.0000 - val_loss: 0.8983 - val_accuracy: 0.8537\n",
      "Epoch 283/300\n",
      "160/160 [==============================] - 0s 747us/sample - loss: 5.5514e-05 - accuracy: 1.0000 - val_loss: 0.8992 - val_accuracy: 0.8537\n",
      "Epoch 284/300\n",
      "160/160 [==============================] - 0s 813us/sample - loss: 5.5018e-05 - accuracy: 1.0000 - val_loss: 0.9012 - val_accuracy: 0.8537\n",
      "Epoch 285/300\n",
      "160/160 [==============================] - 0s 826us/sample - loss: 5.4550e-05 - accuracy: 1.0000 - val_loss: 0.9030 - val_accuracy: 0.8537\n",
      "Epoch 286/300\n",
      "160/160 [==============================] - 0s 679us/sample - loss: 5.4103e-05 - accuracy: 1.0000 - val_loss: 0.9041 - val_accuracy: 0.8537\n",
      "Epoch 287/300\n",
      "160/160 [==============================] - 0s 851us/sample - loss: 5.3746e-05 - accuracy: 1.0000 - val_loss: 0.9052 - val_accuracy: 0.8537\n",
      "Epoch 288/300\n",
      "160/160 [==============================] - 0s 883us/sample - loss: 5.3400e-05 - accuracy: 1.0000 - val_loss: 0.9071 - val_accuracy: 0.8537\n",
      "Epoch 289/300\n",
      "160/160 [==============================] - 0s 843us/sample - loss: 5.2892e-05 - accuracy: 1.0000 - val_loss: 0.9077 - val_accuracy: 0.8537\n",
      "Epoch 290/300\n",
      "160/160 [==============================] - 0s 808us/sample - loss: 5.2456e-05 - accuracy: 1.0000 - val_loss: 0.9079 - val_accuracy: 0.8537\n",
      "Epoch 291/300\n",
      "160/160 [==============================] - 0s 759us/sample - loss: 5.1946e-05 - accuracy: 1.0000 - val_loss: 0.9085 - val_accuracy: 0.8537\n",
      "Epoch 292/300\n",
      "160/160 [==============================] - 0s 721us/sample - loss: 5.1540e-05 - accuracy: 1.0000 - val_loss: 0.9086 - val_accuracy: 0.8537\n",
      "Epoch 293/300\n",
      "160/160 [==============================] - 0s 719us/sample - loss: 5.1109e-05 - accuracy: 1.0000 - val_loss: 0.9094 - val_accuracy: 0.8537\n",
      "Epoch 294/300\n",
      "160/160 [==============================] - 0s 742us/sample - loss: 5.0680e-05 - accuracy: 1.0000 - val_loss: 0.9100 - val_accuracy: 0.8537\n",
      "Epoch 295/300\n",
      "160/160 [==============================] - 0s 735us/sample - loss: 5.0259e-05 - accuracy: 1.0000 - val_loss: 0.9112 - val_accuracy: 0.8537\n",
      "Epoch 296/300\n",
      "160/160 [==============================] - 0s 688us/sample - loss: 4.9841e-05 - accuracy: 1.0000 - val_loss: 0.9124 - val_accuracy: 0.8537\n",
      "Epoch 297/300\n",
      "160/160 [==============================] - 0s 669us/sample - loss: 4.9571e-05 - accuracy: 1.0000 - val_loss: 0.9136 - val_accuracy: 0.8537\n",
      "Epoch 298/300\n",
      "160/160 [==============================] - 0s 639us/sample - loss: 4.9177e-05 - accuracy: 1.0000 - val_loss: 0.9141 - val_accuracy: 0.8537\n",
      "Epoch 299/300\n",
      "160/160 [==============================] - 0s 710us/sample - loss: 4.8771e-05 - accuracy: 1.0000 - val_loss: 0.9134 - val_accuracy: 0.8537\n",
      "Epoch 300/300\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 4.8367e-05 - accuracy: 1.0000 - val_loss: 0.9131 - val_accuracy: 0.8537\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2852b42e0>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(testx,testy), epochs=300, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T17:06:53.259354Z",
     "end_time": "2023-05-28T17:07:32.811721Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 85.37%\n"
     ]
    }
   ],
   "source": [
    "loss_test, accuracy_test = model.evaluate(x_test, y_test)\n",
    "print('Accuracy on test data: {:4.2f}%'.format(accuracy_test * 100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T22:42:59.449993Z",
     "end_time": "2023-05-31T22:42:59.616565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(model=model, clip_values=(0, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T22:42:56.823091Z",
     "end_time": "2023-05-31T22:42:56.934139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from os.path import abspath\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from art.attacks.poisoning.poisoning_attack_svm import PoisoningAttackSVM\n",
    "from art.estimators.classification.scikitlearn import ScikitlearnSVC\n",
    "from art.defences.detector.poison import ProvenanceDefense, RONIDefense\n",
    "from art.utils import load_mnist\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(301)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys\n",
    "from os.path import abspath\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras.backend as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.attacks.poisoning import PoisoningAttackBackdoor, PoisoningAttackCleanLabelBackdoor\n",
    "from art.attacks.poisoning.perturbations import add_pattern_bd\n",
    "from art.utils import load_mnist, preprocess, to_categorical\n",
    "from art.defences.trainer import AdversarialTrainerMadryPGD\n",
    "\n",
    "\n",
    "from art.estimators.classification.deep_partition_ensemble import DeepPartitionEnsemble"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T17:07:33.059498Z",
     "end_time": "2023-05-28T17:07:33.090636Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "def attack(model, attack, x, y):\n",
    "    x_adv = attack.generate(x)\n",
    "    loss, accuracy = model.evaluate(x_adv, y)\n",
    "    print('Accuracy on adversarial test examples: {}%'.format(accuracy * 100))\n",
    "    return x_adv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T22:03:39.140303Z",
     "end_time": "2023-05-31T22:03:39.212658Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 17:07:33.092948: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-28 17:07:33.260530: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 70.73171138763428%\n"
     ]
    }
   ],
   "source": [
    "x_adv = attack(model, FastGradientMethod(estimator=classifier, eps=1/255), x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T17:07:33.096398Z",
     "end_time": "2023-05-28T17:07:33.486347Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 100.0%\n",
      "Accuracy on adversarial test examples: 100.0%\n",
      "Accuracy on adversarial test examples: 98.7500011920929%\n",
      "Accuracy on adversarial test examples: 75.62500238418579%\n",
      "Accuracy on adversarial test examples: 45.625001192092896%\n",
      "Accuracy on adversarial test examples: 25.0%\n",
      "Accuracy on adversarial test examples: 18.75%\n",
      "Accuracy on adversarial test examples: 15.000000596046448%\n",
      "Accuracy on adversarial test examples: 13.750000298023224%\n",
      "Accuracy on adversarial test examples: 10.000000149011612%\n",
      "Accuracy on adversarial test examples: 8.749999850988388%\n",
      "Accuracy on adversarial test examples: 6.25%\n",
      "Accuracy on adversarial test examples: 5.625000223517418%\n",
      "Accuracy on adversarial test examples: 5.625000223517418%\n",
      "Accuracy on adversarial test examples: 4.374999925494194%\n",
      "Accuracy on adversarial test examples: 3.750000149011612%\n",
      "Accuracy on adversarial test examples: 3.125%\n",
      "Accuracy on adversarial test examples: 3.125%\n",
      "Accuracy on adversarial test examples: 3.125%\n",
      "Accuracy on adversarial test examples: 3.125%\n"
     ]
    }
   ],
   "source": [
    "accuracy_fgsm = []\n",
    "eps = np.linspace(1e-3, 20/255, 20)\n",
    "for _eps in eps:\n",
    "    x_adv = attack(model, FastGradientMethod(estimator=classifier, eps=_eps), x_train, y_train)\n",
    "    loss, accuracy = model.evaluate(x_adv, y_train)\n",
    "    accuracy_fgsm.append(accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T17:08:22.597594Z",
     "end_time": "2023-05-28T17:08:28.108826Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def plot(x, y, label, xlabel=\"eps\", ylabel=\"accuracy\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y, label=label)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T17:08:28.098747Z",
     "end_time": "2023-05-28T17:08:28.109172Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(eps, accuracy_fgsm, label=\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T17:08:28.117122Z",
     "end_time": "2023-05-28T17:08:28.308892Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "C&W L_inf:   0%|          | 0/54 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aaac049613254ce0b3af309f24fbd28a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 1.8518518656492233%\n",
      "Accuracy on adversarial test examples: 11.11111119389534%\n"
     ]
    }
   ],
   "source": [
    "attack_cw = CarliniLInfMethod(classifier=classifier,\n",
    "                              max_iter=100,\n",
    "                              learning_rate=0.01,\n",
    "                              initial_const=1e0,\n",
    "                              largest_const=2e0)\n",
    "# modify train data with fgsm attack and retrain the model\n",
    "x_adv = []\n",
    "y_adv_true = []\n",
    "k = 3\n",
    "x_train_adv = attack(model, attack_cw, x_train[::k, :], y_train[::k, :])\n",
    "y_adv_true.append([np.array([0,0,0,0,0,1]) for _ in y_train[::k, :]])\n",
    "x_adv.append(x_train_adv)\n",
    "k = 6\n",
    "x_train_adv = attack(model, FastGradientMethod(estimator=classifier, eps=0.04), x_train[::k, :], y_train[::k, :])\n",
    "y_adv_true.append([np.array([0,0,0,0,0,1]) for _ in y_train[::k, :]])\n",
    "# x_adv.append(x_train_adv)\n",
    "# x_train_adv = attack(model, FastGradientMethod(estimator=classifier, eps=0.04), x_train[::4, :], y_train[::4, :])\n",
    "# y_adv_true.append(y_train[::4, :])\n",
    "# x_adv.append(x_train_adv)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:20:35.067790Z",
     "end_time": "2023-05-29T19:22:40.925056Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "(160, 160)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_adv[0]), len(y_adv_true[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T18:27:44.219122Z",
     "end_time": "2023-05-28T18:27:44.271617Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "(160, 6)"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len([6 for _ in y_train[::k, :]]), y_train[::k, :].shape, np.zeros(y_train[::k, :].shape[1])\n",
    "new_column = np.zeros((160, 1))  #      \n",
    "\n",
    "#  y_train\n",
    "y_train_extended = np.hstack((y_train, new_column))  #  y_train   \n",
    "y_train_extended.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:26:09.314091Z",
     "end_time": "2023-05-29T19:26:09.405051Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "x_add_adv_data = np.array(x_train)\n",
    "y_add_adv_data = np.array(y_train_extended)\n",
    "for i in range(len(x_adv)):\n",
    "    x_add_adv_data = np.append(x_add_adv_data, x_adv[i]).reshape(-1, 62, 47, 1)\n",
    "    y_add_adv_data = np.append(y_add_adv_data, y_adv_true[i]).reshape(-1, 6)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:26:12.542825Z",
     "end_time": "2023-05-29T19:26:12.620375Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "((214, 62, 47, 1), (214, 6))"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_add_adv_data.shape,y_add_adv_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:26:14.791759Z",
     "end_time": "2023-05-29T19:26:14.799163Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "trainx_adv,testx_adv,trainy_adv,testy_adv=train_test_split(x_add_adv_data,y_add_adv_data,test_size=0.2,random_state=1)\n",
    "trainx_mod,testx_mod,trainy_mod,testy_mod=train_test_split(x_train, y_train_extended,test_size=0.2,random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:26:17.634940Z",
     "end_time": "2023-05-29T19:26:17.660752Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 60, 45, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 30, 22, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 20, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 14, 10, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 4480)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 512)               2294272   \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,370,278\n",
      "Trainable params: 2,370,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv2D(32,(3,3), input_shape=(62,47,1), activation='relu'))\n",
    "model1.add(MaxPooling2D(2,2))\n",
    "model1.add(Conv2D(32,(3,3), activation='relu'))\n",
    "model1.add(MaxPooling2D(2,2))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(units=512, activation='relu'))\n",
    "model1.add(Dense(units=128, activation='relu'))\n",
    "model1.add(Dense(units=6, activation='softmax'))\n",
    "model1.summary()\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:26:19.494218Z",
     "end_time": "2023-05-29T19:26:19.622853Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples, validate on 32 samples\n",
      "Epoch 1/100\n",
      "128/128 [==============================] - 0s 1ms/sample - loss: 8.2901e-06 - accuracy: 1.0000 - val_loss: 2.1324 - val_accuracy: 0.5938\n",
      "Epoch 2/100\n",
      "128/128 [==============================] - 0s 752us/sample - loss: 8.1293e-06 - accuracy: 1.0000 - val_loss: 2.1337 - val_accuracy: 0.5938\n",
      "Epoch 3/100\n",
      "128/128 [==============================] - 0s 885us/sample - loss: 7.9518e-06 - accuracy: 1.0000 - val_loss: 2.1257 - val_accuracy: 0.5938\n",
      "Epoch 4/100\n",
      "128/128 [==============================] - 0s 1ms/sample - loss: 7.8214e-06 - accuracy: 1.0000 - val_loss: 2.1311 - val_accuracy: 0.5938\n",
      "Epoch 5/100\n",
      "128/128 [==============================] - 0s 1ms/sample - loss: 7.5858e-06 - accuracy: 1.0000 - val_loss: 2.1316 - val_accuracy: 0.5938\n",
      "Epoch 6/100\n",
      "128/128 [==============================] - 0s 875us/sample - loss: 7.4883e-06 - accuracy: 1.0000 - val_loss: 2.1296 - val_accuracy: 0.5938\n",
      "Epoch 7/100\n",
      "128/128 [==============================] - 0s 813us/sample - loss: 7.2992e-06 - accuracy: 1.0000 - val_loss: 2.1260 - val_accuracy: 0.5938\n",
      "Epoch 8/100\n",
      "128/128 [==============================] - 0s 806us/sample - loss: 7.1694e-06 - accuracy: 1.0000 - val_loss: 2.1237 - val_accuracy: 0.5938\n",
      "Epoch 9/100\n",
      "128/128 [==============================] - 0s 948us/sample - loss: 6.9970e-06 - accuracy: 1.0000 - val_loss: 2.1283 - val_accuracy: 0.5938\n",
      "Epoch 10/100\n",
      "128/128 [==============================] - 0s 1ms/sample - loss: 6.8944e-06 - accuracy: 1.0000 - val_loss: 2.1219 - val_accuracy: 0.5938\n",
      "Epoch 11/100\n",
      "128/128 [==============================] - 0s 1ms/sample - loss: 6.8279e-06 - accuracy: 1.0000 - val_loss: 2.1240 - val_accuracy: 0.5938\n",
      "Epoch 12/100\n",
      "128/128 [==============================] - 0s 977us/sample - loss: 6.6620e-06 - accuracy: 1.0000 - val_loss: 2.1361 - val_accuracy: 0.6250\n",
      "Epoch 13/100\n",
      "128/128 [==============================] - 0s 861us/sample - loss: 6.5710e-06 - accuracy: 1.0000 - val_loss: 2.1250 - val_accuracy: 0.5938\n",
      "Epoch 14/100\n",
      "128/128 [==============================] - 0s 793us/sample - loss: 6.4096e-06 - accuracy: 1.0000 - val_loss: 2.1241 - val_accuracy: 0.5938\n",
      "Epoch 15/100\n",
      "128/128 [==============================] - 0s 909us/sample - loss: 6.3083e-06 - accuracy: 1.0000 - val_loss: 2.1345 - val_accuracy: 0.6250\n",
      "Epoch 16/100\n",
      "128/128 [==============================] - 0s 915us/sample - loss: 6.2230e-06 - accuracy: 1.0000 - val_loss: 2.1246 - val_accuracy: 0.5938\n",
      "Epoch 17/100\n",
      "128/128 [==============================] - 0s 992us/sample - loss: 6.0617e-06 - accuracy: 1.0000 - val_loss: 2.1257 - val_accuracy: 0.5938\n",
      "Epoch 18/100\n",
      "128/128 [==============================] - 0s 902us/sample - loss: 6.0320e-06 - accuracy: 1.0000 - val_loss: 2.1318 - val_accuracy: 0.6250\n",
      "Epoch 19/100\n",
      "128/128 [==============================] - 0s 1ms/sample - loss: 5.8176e-06 - accuracy: 1.0000 - val_loss: 2.1201 - val_accuracy: 0.5938\n",
      "Epoch 20/100\n",
      "128/128 [==============================] - 0s 1ms/sample - loss: 5.7989e-06 - accuracy: 1.0000 - val_loss: 2.1232 - val_accuracy: 0.5938\n",
      "Epoch 21/100\n",
      "128/128 [==============================] - 0s 998us/sample - loss: 5.6343e-06 - accuracy: 1.0000 - val_loss: 2.1276 - val_accuracy: 0.6250\n",
      "Epoch 22/100\n",
      "128/128 [==============================] - 0s 859us/sample - loss: 5.5736e-06 - accuracy: 1.0000 - val_loss: 2.1272 - val_accuracy: 0.6250\n",
      "Epoch 23/100\n",
      "128/128 [==============================] - 0s 854us/sample - loss: 5.4878e-06 - accuracy: 1.0000 - val_loss: 2.1212 - val_accuracy: 0.5938\n",
      "Epoch 24/100\n",
      "128/128 [==============================] - 0s 938us/sample - loss: 5.3819e-06 - accuracy: 1.0000 - val_loss: 2.1287 - val_accuracy: 0.6250\n",
      "Epoch 25/100\n",
      "128/128 [==============================] - 0s 1ms/sample - loss: 5.3141e-06 - accuracy: 1.0000 - val_loss: 2.1307 - val_accuracy: 0.6250\n",
      "Epoch 26/100\n",
      "128/128 [==============================] - 0s 974us/sample - loss: 5.1863e-06 - accuracy: 1.0000 - val_loss: 2.1257 - val_accuracy: 0.5938\n",
      "Epoch 27/100\n",
      "128/128 [==============================] - 0s 957us/sample - loss: 5.1553e-06 - accuracy: 1.0000 - val_loss: 2.1308 - val_accuracy: 0.6250\n",
      "Epoch 28/100\n",
      "128/128 [==============================] - 0s 863us/sample - loss: 5.0210e-06 - accuracy: 1.0000 - val_loss: 2.1306 - val_accuracy: 0.6250\n",
      "Epoch 29/100\n",
      "128/128 [==============================] - 0s 904us/sample - loss: 4.9436e-06 - accuracy: 1.0000 - val_loss: 2.1328 - val_accuracy: 0.6250\n",
      "Epoch 30/100\n",
      "128/128 [==============================] - 0s 899us/sample - loss: 4.8868e-06 - accuracy: 1.0000 - val_loss: 2.1340 - val_accuracy: 0.6250\n",
      "Epoch 31/100\n",
      "128/128 [==============================] - 0s 980us/sample - loss: 4.7854e-06 - accuracy: 1.0000 - val_loss: 2.1302 - val_accuracy: 0.6250\n",
      "Epoch 32/100\n",
      "128/128 [==============================] - 0s 1ms/sample - loss: 4.7518e-06 - accuracy: 1.0000 - val_loss: 2.1317 - val_accuracy: 0.6250\n",
      "Epoch 33/100\n",
      "128/128 [==============================] - 0s 924us/sample - loss: 4.6841e-06 - accuracy: 1.0000 - val_loss: 2.1351 - val_accuracy: 0.6250\n",
      "Epoch 34/100\n",
      "128/128 [==============================] - 0s 880us/sample - loss: 4.6131e-06 - accuracy: 1.0000 - val_loss: 2.1281 - val_accuracy: 0.6250\n",
      "Epoch 35/100\n",
      "128/128 [==============================] - 0s 827us/sample - loss: 4.5646e-06 - accuracy: 1.0000 - val_loss: 2.1322 - val_accuracy: 0.6250\n",
      "Epoch 36/100\n",
      "128/128 [==============================] - 0s 905us/sample - loss: 4.4633e-06 - accuracy: 1.0000 - val_loss: 2.1296 - val_accuracy: 0.6250\n",
      "Epoch 37/100\n",
      "128/128 [==============================] - 0s 908us/sample - loss: 4.3949e-06 - accuracy: 1.0000 - val_loss: 2.1309 - val_accuracy: 0.6250\n",
      "Epoch 38/100\n",
      "128/128 [==============================] - 0s 981us/sample - loss: 4.3471e-06 - accuracy: 1.0000 - val_loss: 2.1306 - val_accuracy: 0.6250\n",
      "Epoch 39/100\n",
      "128/128 [==============================] - 0s 942us/sample - loss: 4.3613e-06 - accuracy: 1.0000 - val_loss: 2.1397 - val_accuracy: 0.6250\n",
      "Epoch 40/100\n",
      "128/128 [==============================] - 0s 1ms/sample - loss: 4.2645e-06 - accuracy: 1.0000 - val_loss: 2.1286 - val_accuracy: 0.6250\n",
      "Epoch 41/100\n",
      "128/128 [==============================] - 0s 757us/sample - loss: 4.1870e-06 - accuracy: 1.0000 - val_loss: 2.1327 - val_accuracy: 0.6250\n",
      "Epoch 42/100\n",
      "128/128 [==============================] - 0s 829us/sample - loss: 4.1018e-06 - accuracy: 1.0000 - val_loss: 2.1328 - val_accuracy: 0.6250\n",
      "Epoch 43/100\n",
      "128/128 [==============================] - 0s 960us/sample - loss: 4.0430e-06 - accuracy: 1.0000 - val_loss: 2.1323 - val_accuracy: 0.6250\n",
      "Epoch 44/100\n",
      "128/128 [==============================] - 0s 1ms/sample - loss: 3.9772e-06 - accuracy: 1.0000 - val_loss: 2.1345 - val_accuracy: 0.6250\n",
      "Epoch 45/100\n",
      "128/128 [==============================] - 0s 1ms/sample - loss: 3.9191e-06 - accuracy: 1.0000 - val_loss: 2.1372 - val_accuracy: 0.6250\n",
      "Epoch 46/100\n",
      "128/128 [==============================] - 0s 872us/sample - loss: 3.9010e-06 - accuracy: 1.0000 - val_loss: 2.1412 - val_accuracy: 0.6250\n",
      "Epoch 47/100\n",
      "128/128 [==============================] - 0s 944us/sample - loss: 3.8216e-06 - accuracy: 1.0000 - val_loss: 2.1365 - val_accuracy: 0.6250\n",
      "Epoch 48/100\n",
      "128/128 [==============================] - 0s 935us/sample - loss: 3.7809e-06 - accuracy: 1.0000 - val_loss: 2.1373 - val_accuracy: 0.6250\n",
      "Epoch 49/100\n",
      "128/128 [==============================] - 0s 875us/sample - loss: 3.7293e-06 - accuracy: 1.0000 - val_loss: 2.1379 - val_accuracy: 0.6250\n",
      "Epoch 50/100\n",
      "128/128 [==============================] - 0s 900us/sample - loss: 3.6802e-06 - accuracy: 1.0000 - val_loss: 2.1420 - val_accuracy: 0.6250\n",
      "Epoch 51/100\n",
      "128/128 [==============================] - 0s 914us/sample - loss: 3.6312e-06 - accuracy: 1.0000 - val_loss: 2.1389 - val_accuracy: 0.6250\n",
      "Epoch 52/100\n",
      "128/128 [==============================] - 0s 804us/sample - loss: 3.6067e-06 - accuracy: 1.0000 - val_loss: 2.1410 - val_accuracy: 0.6250\n",
      "Epoch 53/100\n",
      "128/128 [==============================] - 0s 801us/sample - loss: 3.5395e-06 - accuracy: 1.0000 - val_loss: 2.1481 - val_accuracy: 0.6250\n",
      "Epoch 54/100\n",
      "128/128 [==============================] - 0s 901us/sample - loss: 3.5221e-06 - accuracy: 1.0000 - val_loss: 2.1426 - val_accuracy: 0.6250\n",
      "Epoch 55/100\n",
      "128/128 [==============================] - 0s 941us/sample - loss: 3.4504e-06 - accuracy: 1.0000 - val_loss: 2.1444 - val_accuracy: 0.6250\n",
      "Epoch 56/100\n",
      "128/128 [==============================] - 0s 887us/sample - loss: 3.4078e-06 - accuracy: 1.0000 - val_loss: 2.1460 - val_accuracy: 0.6250\n",
      "Epoch 57/100\n",
      "128/128 [==============================] - 0s 938us/sample - loss: 3.3736e-06 - accuracy: 1.0000 - val_loss: 2.1466 - val_accuracy: 0.6250\n",
      "Epoch 58/100\n",
      "128/128 [==============================] - 0s 868us/sample - loss: 3.3162e-06 - accuracy: 1.0000 - val_loss: 2.1485 - val_accuracy: 0.6250\n",
      "Epoch 59/100\n",
      "128/128 [==============================] - 0s 830us/sample - loss: 3.2826e-06 - accuracy: 1.0000 - val_loss: 2.1476 - val_accuracy: 0.6250\n",
      "Epoch 60/100\n",
      "128/128 [==============================] - 0s 898us/sample - loss: 3.2600e-06 - accuracy: 1.0000 - val_loss: 2.1517 - val_accuracy: 0.6250\n",
      "Epoch 61/100\n",
      "128/128 [==============================] - 0s 934us/sample - loss: 3.1961e-06 - accuracy: 1.0000 - val_loss: 2.1519 - val_accuracy: 0.6250\n",
      "Epoch 62/100\n",
      "128/128 [==============================] - 0s 839us/sample - loss: 3.1574e-06 - accuracy: 1.0000 - val_loss: 2.1533 - val_accuracy: 0.6250\n",
      "Epoch 63/100\n",
      "128/128 [==============================] - 0s 898us/sample - loss: 3.1490e-06 - accuracy: 1.0000 - val_loss: 2.1537 - val_accuracy: 0.6250\n",
      "Epoch 64/100\n",
      "128/128 [==============================] - 0s 871us/sample - loss: 3.0902e-06 - accuracy: 1.0000 - val_loss: 2.1607 - val_accuracy: 0.6250\n",
      "Epoch 65/100\n",
      "128/128 [==============================] - 0s 885us/sample - loss: 3.0715e-06 - accuracy: 1.0000 - val_loss: 2.1566 - val_accuracy: 0.6250\n",
      "Epoch 66/100\n",
      "128/128 [==============================] - 0s 911us/sample - loss: 3.0147e-06 - accuracy: 1.0000 - val_loss: 2.1539 - val_accuracy: 0.6250\n",
      "Epoch 67/100\n",
      "128/128 [==============================] - 0s 849us/sample - loss: 2.9695e-06 - accuracy: 1.0000 - val_loss: 2.1575 - val_accuracy: 0.6250\n",
      "Epoch 68/100\n",
      "128/128 [==============================] - 0s 859us/sample - loss: 2.9463e-06 - accuracy: 1.0000 - val_loss: 2.1564 - val_accuracy: 0.6250\n",
      "Epoch 69/100\n",
      "128/128 [==============================] - 0s 919us/sample - loss: 2.9049e-06 - accuracy: 1.0000 - val_loss: 2.1544 - val_accuracy: 0.6250\n",
      "Epoch 70/100\n",
      "128/128 [==============================] - 0s 945us/sample - loss: 2.8720e-06 - accuracy: 1.0000 - val_loss: 2.1552 - val_accuracy: 0.6250\n",
      "Epoch 71/100\n",
      "128/128 [==============================] - 0s 858us/sample - loss: 2.8423e-06 - accuracy: 1.0000 - val_loss: 2.1598 - val_accuracy: 0.6250\n",
      "Epoch 72/100\n",
      "128/128 [==============================] - 0s 846us/sample - loss: 2.7984e-06 - accuracy: 1.0000 - val_loss: 2.1582 - val_accuracy: 0.6250\n",
      "Epoch 73/100\n",
      "128/128 [==============================] - 0s 927us/sample - loss: 2.7862e-06 - accuracy: 1.0000 - val_loss: 2.1591 - val_accuracy: 0.6250\n",
      "Epoch 74/100\n",
      "128/128 [==============================] - 0s 978us/sample - loss: 2.7313e-06 - accuracy: 1.0000 - val_loss: 2.1619 - val_accuracy: 0.6250\n",
      "Epoch 75/100\n",
      "128/128 [==============================] - 0s 909us/sample - loss: 2.6893e-06 - accuracy: 1.0000 - val_loss: 2.1642 - val_accuracy: 0.6250\n",
      "Epoch 76/100\n",
      "128/128 [==============================] - 0s 889us/sample - loss: 2.6693e-06 - accuracy: 1.0000 - val_loss: 2.1652 - val_accuracy: 0.6250\n",
      "Epoch 77/100\n",
      "128/128 [==============================] - 0s 925us/sample - loss: 2.6306e-06 - accuracy: 1.0000 - val_loss: 2.1644 - val_accuracy: 0.6250\n",
      "Epoch 78/100\n",
      "128/128 [==============================] - 0s 889us/sample - loss: 2.5996e-06 - accuracy: 1.0000 - val_loss: 2.1672 - val_accuracy: 0.6250\n",
      "Epoch 79/100\n",
      "128/128 [==============================] - 0s 926us/sample - loss: 2.5673e-06 - accuracy: 1.0000 - val_loss: 2.1677 - val_accuracy: 0.6250\n",
      "Epoch 80/100\n",
      "128/128 [==============================] - 0s 957us/sample - loss: 2.5434e-06 - accuracy: 1.0000 - val_loss: 2.1681 - val_accuracy: 0.6250\n",
      "Epoch 81/100\n",
      "128/128 [==============================] - 0s 918us/sample - loss: 2.5241e-06 - accuracy: 1.0000 - val_loss: 2.1716 - val_accuracy: 0.6250\n",
      "Epoch 82/100\n",
      "128/128 [==============================] - 0s 866us/sample - loss: 2.4912e-06 - accuracy: 1.0000 - val_loss: 2.1690 - val_accuracy: 0.6250\n",
      "Epoch 83/100\n",
      "128/128 [==============================] - 0s 872us/sample - loss: 2.4556e-06 - accuracy: 1.0000 - val_loss: 2.1692 - val_accuracy: 0.6250\n",
      "Epoch 84/100\n",
      "128/128 [==============================] - 0s 881us/sample - loss: 2.4298e-06 - accuracy: 1.0000 - val_loss: 2.1714 - val_accuracy: 0.6250\n",
      "Epoch 85/100\n",
      "128/128 [==============================] - 0s 912us/sample - loss: 2.4130e-06 - accuracy: 1.0000 - val_loss: 2.1713 - val_accuracy: 0.6250\n",
      "Epoch 86/100\n",
      "128/128 [==============================] - 0s 930us/sample - loss: 2.3911e-06 - accuracy: 1.0000 - val_loss: 2.1744 - val_accuracy: 0.6250\n",
      "Epoch 87/100\n",
      "128/128 [==============================] - 0s 884us/sample - loss: 2.3595e-06 - accuracy: 1.0000 - val_loss: 2.1738 - val_accuracy: 0.6250\n",
      "Epoch 88/100\n",
      "128/128 [==============================] - 0s 857us/sample - loss: 2.3278e-06 - accuracy: 1.0000 - val_loss: 2.1742 - val_accuracy: 0.6250\n",
      "Epoch 89/100\n",
      "128/128 [==============================] - 0s 864us/sample - loss: 2.3001e-06 - accuracy: 1.0000 - val_loss: 2.1774 - val_accuracy: 0.6250\n",
      "Epoch 90/100\n",
      "128/128 [==============================] - 0s 932us/sample - loss: 2.2794e-06 - accuracy: 1.0000 - val_loss: 2.1819 - val_accuracy: 0.6250\n",
      "Epoch 91/100\n",
      "128/128 [==============================] - 0s 930us/sample - loss: 2.2665e-06 - accuracy: 1.0000 - val_loss: 2.1797 - val_accuracy: 0.6250\n",
      "Epoch 92/100\n",
      "128/128 [==============================] - 0s 937us/sample - loss: 2.2510e-06 - accuracy: 1.0000 - val_loss: 2.1792 - val_accuracy: 0.6250\n",
      "Epoch 93/100\n",
      "128/128 [==============================] - 0s 896us/sample - loss: 2.2084e-06 - accuracy: 1.0000 - val_loss: 2.1844 - val_accuracy: 0.6250\n",
      "Epoch 94/100\n",
      "128/128 [==============================] - 0s 882us/sample - loss: 2.1852e-06 - accuracy: 1.0000 - val_loss: 2.1872 - val_accuracy: 0.6250\n",
      "Epoch 95/100\n",
      "128/128 [==============================] - 0s 827us/sample - loss: 2.1626e-06 - accuracy: 1.0000 - val_loss: 2.1834 - val_accuracy: 0.6250\n",
      "Epoch 96/100\n",
      "128/128 [==============================] - 0s 857us/sample - loss: 2.1445e-06 - accuracy: 1.0000 - val_loss: 2.1851 - val_accuracy: 0.6250\n",
      "Epoch 97/100\n",
      "128/128 [==============================] - 0s 899us/sample - loss: 2.1045e-06 - accuracy: 1.0000 - val_loss: 2.1921 - val_accuracy: 0.6250\n",
      "Epoch 98/100\n",
      "128/128 [==============================] - 0s 847us/sample - loss: 2.0896e-06 - accuracy: 1.0000 - val_loss: 2.1912 - val_accuracy: 0.6250\n",
      "Epoch 99/100\n",
      "128/128 [==============================] - 0s 886us/sample - loss: 2.0670e-06 - accuracy: 1.0000 - val_loss: 2.1889 - val_accuracy: 0.6250\n",
      "Epoch 100/100\n",
      "128/128 [==============================] - 0s 879us/sample - loss: 2.0438e-06 - accuracy: 1.0000 - val_loss: 2.1928 - val_accuracy: 0.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2d6e3b700>"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(trainx_mod,trainy_mod, validation_data=(testx_mod,testy_mod),epochs=300, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:28:45.165800Z",
     "end_time": "2023-05-29T19:28:57.361733Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 19:26:59.276553: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 100.0%\n",
      "Accuracy on adversarial test examples: 100.0%\n",
      "Accuracy on adversarial test examples: 100.0%\n",
      "Accuracy on adversarial test examples: 98.4375%\n",
      "Accuracy on adversarial test examples: 91.40625%\n",
      "Accuracy on adversarial test examples: 75.78125%\n",
      "Accuracy on adversarial test examples: 62.5%\n",
      "Accuracy on adversarial test examples: 47.65625%\n",
      "Accuracy on adversarial test examples: 39.84375%\n",
      "Accuracy on adversarial test examples: 32.03125%\n",
      "Accuracy on adversarial test examples: 23.4375%\n",
      "Accuracy on adversarial test examples: 17.1875%\n",
      "Accuracy on adversarial test examples: 14.84375%\n",
      "Accuracy on adversarial test examples: 13.28125%\n",
      "Accuracy on adversarial test examples: 10.9375%\n",
      "Accuracy on adversarial test examples: 10.9375%\n",
      "Accuracy on adversarial test examples: 9.375%\n",
      "Accuracy on adversarial test examples: 9.375%\n",
      "Accuracy on adversarial test examples: 7.03125%\n"
     ]
    }
   ],
   "source": [
    "eps = np.linspace(1e-4, 20/255, 20)\n",
    "accuracy_fgsm = []\n",
    "for _eps in eps:\n",
    "    x_adv = attack(model1, FastGradientMethod(estimator=classifier, eps=_eps), trainx_mod, trainy_mod)\n",
    "    y_pred = model1.predict(x_adv)\n",
    "    accuracy_fgsm.append(sum([np.argmax(y_pred[i]) == np.argmax(trainy_mod[i]) for i in range(y_pred.shape[0])]) / y_pred.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:26:59.104900Z",
     "end_time": "2023-05-29T19:27:02.283171Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "[1.0,\n 1.0,\n 1.0,\n 1.0,\n 0.984375,\n 0.9140625,\n 0.7578125,\n 0.625,\n 0.4765625,\n 0.3984375,\n 0.3203125,\n 0.234375,\n 0.171875,\n 0.1484375,\n 0.1328125,\n 0.109375,\n 0.109375,\n 0.09375,\n 0.09375,\n 0.0703125]"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fgsm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:28:01.797832Z",
     "end_time": "2023-05-29T19:28:01.805345Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 171 samples, validate on 43 samples\n",
      "Epoch 1/30\n",
      "171/171 [==============================] - 0s 2ms/sample - loss: 4.1741 - accuracy: 0.4912 - val_loss: 1.5497 - val_accuracy: 0.4186\n",
      "Epoch 2/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 1.5231 - accuracy: 0.3977 - val_loss: 1.5579 - val_accuracy: 0.4186\n",
      "Epoch 3/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 1.4733 - accuracy: 0.3918 - val_loss: 1.4933 - val_accuracy: 0.3953\n",
      "Epoch 4/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 1.2997 - accuracy: 0.5263 - val_loss: 1.4535 - val_accuracy: 0.3953\n",
      "Epoch 5/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 1.1533 - accuracy: 0.5146 - val_loss: 1.3381 - val_accuracy: 0.4884\n",
      "Epoch 6/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 1.0424 - accuracy: 0.5088 - val_loss: 1.4486 - val_accuracy: 0.4186\n",
      "Epoch 7/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.8638 - accuracy: 0.6374 - val_loss: 1.4049 - val_accuracy: 0.4186\n",
      "Epoch 8/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.6538 - accuracy: 0.6901 - val_loss: 1.4518 - val_accuracy: 0.4419\n",
      "Epoch 9/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.5431 - accuracy: 0.8129 - val_loss: 1.2895 - val_accuracy: 0.4884\n",
      "Epoch 10/30\n",
      "171/171 [==============================] - 0s 884us/sample - loss: 0.4960 - accuracy: 0.8246 - val_loss: 1.5280 - val_accuracy: 0.5349\n",
      "Epoch 11/30\n",
      "171/171 [==============================] - 0s 921us/sample - loss: 0.4849 - accuracy: 0.7661 - val_loss: 1.6853 - val_accuracy: 0.3953\n",
      "Epoch 12/30\n",
      "171/171 [==============================] - 0s 928us/sample - loss: 0.3155 - accuracy: 0.9064 - val_loss: 1.5933 - val_accuracy: 0.5349\n",
      "Epoch 13/30\n",
      "171/171 [==============================] - 0s 1000us/sample - loss: 0.3456 - accuracy: 0.8596 - val_loss: 1.6433 - val_accuracy: 0.5116\n",
      "Epoch 14/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.2478 - accuracy: 0.9123 - val_loss: 1.7007 - val_accuracy: 0.4884\n",
      "Epoch 15/30\n",
      "171/171 [==============================] - 0s 986us/sample - loss: 0.2598 - accuracy: 0.8889 - val_loss: 1.7070 - val_accuracy: 0.4884\n",
      "Epoch 16/30\n",
      "171/171 [==============================] - 0s 967us/sample - loss: 0.1629 - accuracy: 0.9357 - val_loss: 1.7490 - val_accuracy: 0.4884\n",
      "Epoch 17/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.2764 - accuracy: 0.8830 - val_loss: 2.8203 - val_accuracy: 0.3721\n",
      "Epoch 18/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.2743 - accuracy: 0.8830 - val_loss: 1.8531 - val_accuracy: 0.5116\n",
      "Epoch 19/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.2900 - accuracy: 0.8655 - val_loss: 1.5919 - val_accuracy: 0.5116\n",
      "Epoch 20/30\n",
      "171/171 [==============================] - 0s 974us/sample - loss: 0.2496 - accuracy: 0.8947 - val_loss: 1.9887 - val_accuracy: 0.4186\n",
      "Epoch 21/30\n",
      "171/171 [==============================] - 0s 952us/sample - loss: 0.1576 - accuracy: 0.9591 - val_loss: 1.8909 - val_accuracy: 0.4651\n",
      "Epoch 22/30\n",
      "171/171 [==============================] - 0s 964us/sample - loss: 0.1290 - accuracy: 0.9532 - val_loss: 2.0421 - val_accuracy: 0.4419\n",
      "Epoch 23/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.1051 - accuracy: 0.9766 - val_loss: 2.1139 - val_accuracy: 0.4419\n",
      "Epoch 24/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0879 - accuracy: 0.9883 - val_loss: 2.1592 - val_accuracy: 0.4884\n",
      "Epoch 25/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.1272 - accuracy: 0.9591 - val_loss: 2.3048 - val_accuracy: 0.4651\n",
      "Epoch 26/30\n",
      "171/171 [==============================] - 0s 993us/sample - loss: 0.1821 - accuracy: 0.9006 - val_loss: 2.5344 - val_accuracy: 0.4419\n",
      "Epoch 27/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.2066 - accuracy: 0.9240 - val_loss: 2.8800 - val_accuracy: 0.3721\n",
      "Epoch 28/30\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.1391 - accuracy: 0.9532 - val_loss: 2.2204 - val_accuracy: 0.5116\n",
      "Epoch 29/30\n",
      "171/171 [==============================] - 0s 957us/sample - loss: 0.1443 - accuracy: 0.9532 - val_loss: 2.4326 - val_accuracy: 0.3953\n",
      "Epoch 30/30\n",
      "171/171 [==============================] - 0s 965us/sample - loss: 0.1276 - accuracy: 0.9532 - val_loss: 2.0983 - val_accuracy: 0.3953\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2d6b8b6d0>"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(trainx_adv,trainy_adv, validation_data=(testx_adv,testy_adv),epochs=30, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:29:01.917136Z",
     "end_time": "2023-05-29T19:29:07.542765Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "data": {
      "text/plain": "((374, 62, 47, 1), (374, 5))"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_add_adv_data = np.append(x_train, x_train_adv).reshape(-1, 62, 47, 1)\n",
    "# y_add_adv_data = np.append(y_train, y_train[::3, :]).reshape(-1, 5)\n",
    "x_add_adv_data.shape, y_add_adv_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-22T18:33:29.507202Z",
     "end_time": "2023-05-22T18:33:29.510015Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples, validate on 64 samples\n",
      "Epoch 1/300\n",
      "256/256 [==============================] - 0s 910us/sample - loss: 0.4298 - accuracy: 0.8359 - val_loss: 1.1833 - val_accuracy: 0.5625\n",
      "Epoch 2/300\n",
      "256/256 [==============================] - 0s 1ms/sample - loss: 0.4211 - accuracy: 0.8359 - val_loss: 1.2837 - val_accuracy: 0.5938\n",
      "Epoch 3/300\n",
      "256/256 [==============================] - 0s 976us/sample - loss: 0.3711 - accuracy: 0.8633 - val_loss: 1.2442 - val_accuracy: 0.5625\n",
      "Epoch 4/300\n",
      "256/256 [==============================] - 0s 784us/sample - loss: 0.2902 - accuracy: 0.9297 - val_loss: 1.2604 - val_accuracy: 0.6719\n",
      "Epoch 5/300\n",
      "256/256 [==============================] - 0s 984us/sample - loss: 0.2408 - accuracy: 0.9297 - val_loss: 1.2236 - val_accuracy: 0.5469\n",
      "Epoch 6/300\n",
      "256/256 [==============================] - 0s 919us/sample - loss: 0.2432 - accuracy: 0.9375 - val_loss: 1.2647 - val_accuracy: 0.6719\n",
      "Epoch 7/300\n",
      "256/256 [==============================] - 0s 849us/sample - loss: 0.2428 - accuracy: 0.9219 - val_loss: 1.5109 - val_accuracy: 0.5469\n",
      "Epoch 8/300\n",
      "256/256 [==============================] - 0s 913us/sample - loss: 0.2210 - accuracy: 0.9336 - val_loss: 1.2530 - val_accuracy: 0.6719\n",
      "Epoch 9/300\n",
      "256/256 [==============================] - 0s 773us/sample - loss: 0.1504 - accuracy: 0.9688 - val_loss: 1.3480 - val_accuracy: 0.6562\n",
      "Epoch 10/300\n",
      "256/256 [==============================] - 0s 869us/sample - loss: 0.1512 - accuracy: 0.9492 - val_loss: 1.3220 - val_accuracy: 0.6250\n",
      "Epoch 11/300\n",
      "256/256 [==============================] - 0s 842us/sample - loss: 0.1504 - accuracy: 0.9570 - val_loss: 1.4977 - val_accuracy: 0.7188\n",
      "Epoch 12/300\n",
      "256/256 [==============================] - 0s 825us/sample - loss: 0.1771 - accuracy: 0.9375 - val_loss: 1.5146 - val_accuracy: 0.5781\n",
      "Epoch 13/300\n",
      "256/256 [==============================] - 0s 802us/sample - loss: 0.1712 - accuracy: 0.9297 - val_loss: 1.3548 - val_accuracy: 0.6562\n",
      "Epoch 14/300\n",
      "256/256 [==============================] - 0s 822us/sample - loss: 0.1349 - accuracy: 0.9453 - val_loss: 1.4272 - val_accuracy: 0.6562\n",
      "Epoch 15/300\n",
      "256/256 [==============================] - 0s 798us/sample - loss: 0.2334 - accuracy: 0.9023 - val_loss: 1.3370 - val_accuracy: 0.5938\n",
      "Epoch 16/300\n",
      "256/256 [==============================] - 0s 809us/sample - loss: 0.2314 - accuracy: 0.8906 - val_loss: 1.7035 - val_accuracy: 0.6250\n",
      "Epoch 17/300\n",
      "256/256 [==============================] - 0s 820us/sample - loss: 0.1331 - accuracy: 0.9766 - val_loss: 1.6079 - val_accuracy: 0.6406\n",
      "Epoch 18/300\n",
      "256/256 [==============================] - 0s 799us/sample - loss: 0.0872 - accuracy: 0.9844 - val_loss: 1.5595 - val_accuracy: 0.6719\n",
      "Epoch 19/300\n",
      "256/256 [==============================] - 0s 786us/sample - loss: 0.0738 - accuracy: 0.9883 - val_loss: 1.7616 - val_accuracy: 0.6250\n",
      "Epoch 20/300\n",
      "256/256 [==============================] - 0s 799us/sample - loss: 0.0862 - accuracy: 0.9727 - val_loss: 1.4585 - val_accuracy: 0.7031\n",
      "Epoch 21/300\n",
      "256/256 [==============================] - 0s 801us/sample - loss: 0.0835 - accuracy: 0.9766 - val_loss: 1.7535 - val_accuracy: 0.5938\n",
      "Epoch 22/300\n",
      "256/256 [==============================] - 0s 811us/sample - loss: 0.0541 - accuracy: 0.9922 - val_loss: 1.7275 - val_accuracy: 0.6406\n",
      "Epoch 23/300\n",
      "256/256 [==============================] - 0s 803us/sample - loss: 0.0625 - accuracy: 0.9922 - val_loss: 1.5579 - val_accuracy: 0.6719\n",
      "Epoch 24/300\n",
      "256/256 [==============================] - 0s 816us/sample - loss: 0.0564 - accuracy: 0.9883 - val_loss: 1.7644 - val_accuracy: 0.7188\n",
      "Epoch 25/300\n",
      "256/256 [==============================] - 0s 882us/sample - loss: 0.0596 - accuracy: 0.9883 - val_loss: 1.7434 - val_accuracy: 0.7031\n",
      "Epoch 26/300\n",
      "256/256 [==============================] - 0s 842us/sample - loss: 0.0563 - accuracy: 0.9844 - val_loss: 1.5657 - val_accuracy: 0.6719\n",
      "Epoch 27/300\n",
      "256/256 [==============================] - 0s 776us/sample - loss: 0.0527 - accuracy: 0.9922 - val_loss: 1.9401 - val_accuracy: 0.7031\n",
      "Epoch 28/300\n",
      "256/256 [==============================] - 0s 843us/sample - loss: 0.0393 - accuracy: 0.9961 - val_loss: 1.6407 - val_accuracy: 0.7031\n",
      "Epoch 29/300\n",
      "256/256 [==============================] - 0s 941us/sample - loss: 0.0519 - accuracy: 0.9922 - val_loss: 1.7991 - val_accuracy: 0.6875\n",
      "Epoch 30/300\n",
      "256/256 [==============================] - 0s 977us/sample - loss: 0.0372 - accuracy: 0.9922 - val_loss: 1.7169 - val_accuracy: 0.7344\n",
      "Epoch 31/300\n",
      "256/256 [==============================] - 0s 781us/sample - loss: 0.0359 - accuracy: 0.9961 - val_loss: 2.0435 - val_accuracy: 0.6250\n",
      "Epoch 32/300\n",
      "256/256 [==============================] - 0s 847us/sample - loss: 0.0596 - accuracy: 0.9844 - val_loss: 1.7977 - val_accuracy: 0.7188\n",
      "Epoch 33/300\n",
      "256/256 [==============================] - 0s 928us/sample - loss: 0.0759 - accuracy: 0.9727 - val_loss: 1.9936 - val_accuracy: 0.5625\n",
      "Epoch 34/300\n",
      "256/256 [==============================] - 0s 807us/sample - loss: 0.1352 - accuracy: 0.9492 - val_loss: 2.2208 - val_accuracy: 0.6562\n",
      "Epoch 35/300\n",
      "256/256 [==============================] - 0s 834us/sample - loss: 0.2283 - accuracy: 0.9102 - val_loss: 1.7474 - val_accuracy: 0.6719\n",
      "Epoch 36/300\n",
      "256/256 [==============================] - 0s 813us/sample - loss: 0.3270 - accuracy: 0.8477 - val_loss: 1.7111 - val_accuracy: 0.5625\n",
      "Epoch 37/300\n",
      "256/256 [==============================] - 0s 842us/sample - loss: 0.2369 - accuracy: 0.8945 - val_loss: 1.4962 - val_accuracy: 0.7031\n",
      "Epoch 38/300\n",
      "256/256 [==============================] - 0s 829us/sample - loss: 0.1377 - accuracy: 0.9609 - val_loss: 1.4325 - val_accuracy: 0.6562\n",
      "Epoch 39/300\n",
      "256/256 [==============================] - 0s 800us/sample - loss: 0.0901 - accuracy: 0.9805 - val_loss: 1.4128 - val_accuracy: 0.7500\n",
      "Epoch 40/300\n",
      "256/256 [==============================] - 0s 804us/sample - loss: 0.0531 - accuracy: 0.9922 - val_loss: 1.9128 - val_accuracy: 0.6562\n",
      "Epoch 41/300\n",
      "256/256 [==============================] - 0s 860us/sample - loss: 0.0347 - accuracy: 1.0000 - val_loss: 1.5557 - val_accuracy: 0.7812\n",
      "Epoch 42/300\n",
      "256/256 [==============================] - 0s 827us/sample - loss: 0.0343 - accuracy: 0.9922 - val_loss: 1.7872 - val_accuracy: 0.7188\n",
      "Epoch 43/300\n",
      "256/256 [==============================] - 0s 805us/sample - loss: 0.0366 - accuracy: 0.9922 - val_loss: 1.7452 - val_accuracy: 0.7500\n",
      "Epoch 44/300\n",
      "256/256 [==============================] - 0s 819us/sample - loss: 0.0201 - accuracy: 0.9961 - val_loss: 1.8635 - val_accuracy: 0.7031\n",
      "Epoch 45/300\n",
      "256/256 [==============================] - 0s 781us/sample - loss: 0.0326 - accuracy: 0.9922 - val_loss: 1.8204 - val_accuracy: 0.7500\n",
      "Epoch 46/300\n",
      "256/256 [==============================] - 0s 837us/sample - loss: 0.0351 - accuracy: 0.9961 - val_loss: 1.7345 - val_accuracy: 0.7656\n",
      "Epoch 47/300\n",
      "256/256 [==============================] - 0s 806us/sample - loss: 0.0239 - accuracy: 0.9961 - val_loss: 1.7971 - val_accuracy: 0.7656\n",
      "Epoch 48/300\n",
      "256/256 [==============================] - 0s 804us/sample - loss: 0.0350 - accuracy: 0.9922 - val_loss: 1.7913 - val_accuracy: 0.7188\n",
      "Epoch 49/300\n",
      "256/256 [==============================] - 0s 801us/sample - loss: 0.0305 - accuracy: 0.9961 - val_loss: 1.9700 - val_accuracy: 0.7031\n",
      "Epoch 50/300\n",
      "256/256 [==============================] - 0s 859us/sample - loss: 0.0240 - accuracy: 0.9961 - val_loss: 1.8992 - val_accuracy: 0.7500\n",
      "Epoch 51/300\n",
      "256/256 [==============================] - 0s 792us/sample - loss: 0.0399 - accuracy: 0.9961 - val_loss: 2.1043 - val_accuracy: 0.6719\n",
      "Epoch 52/300\n",
      "256/256 [==============================] - 0s 803us/sample - loss: 0.0405 - accuracy: 0.9922 - val_loss: 1.8211 - val_accuracy: 0.7969\n",
      "Epoch 53/300\n",
      "256/256 [==============================] - 0s 824us/sample - loss: 0.0274 - accuracy: 0.9961 - val_loss: 2.1859 - val_accuracy: 0.6875\n",
      "Epoch 54/300\n",
      "256/256 [==============================] - 0s 795us/sample - loss: 0.0426 - accuracy: 0.9922 - val_loss: 1.9576 - val_accuracy: 0.7344\n",
      "Epoch 55/300\n",
      "256/256 [==============================] - 0s 787us/sample - loss: 0.0241 - accuracy: 0.9961 - val_loss: 2.1178 - val_accuracy: 0.7344\n",
      "Epoch 56/300\n",
      "256/256 [==============================] - 0s 1ms/sample - loss: 0.0273 - accuracy: 0.9922 - val_loss: 1.7992 - val_accuracy: 0.7656\n",
      "Epoch 57/300\n",
      "256/256 [==============================] - 0s 922us/sample - loss: 0.0219 - accuracy: 0.9961 - val_loss: 2.2530 - val_accuracy: 0.7188\n",
      "Epoch 58/300\n",
      "256/256 [==============================] - 0s 788us/sample - loss: 0.0243 - accuracy: 0.9922 - val_loss: 1.9799 - val_accuracy: 0.7500\n",
      "Epoch 59/300\n",
      "256/256 [==============================] - 0s 827us/sample - loss: 0.0132 - accuracy: 0.9961 - val_loss: 2.0210 - val_accuracy: 0.7188\n",
      "Epoch 60/300\n",
      "256/256 [==============================] - 0s 843us/sample - loss: 0.0179 - accuracy: 0.9961 - val_loss: 2.0299 - val_accuracy: 0.7656\n",
      "Epoch 61/300\n",
      "256/256 [==============================] - 0s 783us/sample - loss: 0.0178 - accuracy: 0.9961 - val_loss: 2.1879 - val_accuracy: 0.6875\n",
      "Epoch 62/300\n",
      "256/256 [==============================] - 0s 813us/sample - loss: 0.0279 - accuracy: 0.9922 - val_loss: 2.0833 - val_accuracy: 0.7344\n",
      "Epoch 63/300\n",
      "256/256 [==============================] - 0s 814us/sample - loss: 0.0162 - accuracy: 0.9961 - val_loss: 1.9361 - val_accuracy: 0.7344\n",
      "Epoch 64/300\n",
      "256/256 [==============================] - 0s 785us/sample - loss: 0.0294 - accuracy: 0.9961 - val_loss: 2.3963 - val_accuracy: 0.7188\n",
      "Epoch 65/300\n",
      "256/256 [==============================] - 0s 830us/sample - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.9113 - val_accuracy: 0.7812\n",
      "Epoch 66/300\n",
      "256/256 [==============================] - 0s 831us/sample - loss: 0.0274 - accuracy: 0.9922 - val_loss: 2.0329 - val_accuracy: 0.7188\n",
      "Epoch 67/300\n",
      "256/256 [==============================] - 0s 1ms/sample - loss: 0.0156 - accuracy: 0.9961 - val_loss: 2.1018 - val_accuracy: 0.7344\n",
      "Epoch 68/300\n",
      "256/256 [==============================] - 0s 805us/sample - loss: 0.0193 - accuracy: 0.9961 - val_loss: 2.2628 - val_accuracy: 0.7344\n",
      "Epoch 69/300\n",
      "256/256 [==============================] - 0s 842us/sample - loss: 0.0178 - accuracy: 0.9922 - val_loss: 1.9668 - val_accuracy: 0.7188\n",
      "Epoch 70/300\n",
      "256/256 [==============================] - 0s 903us/sample - loss: 0.0206 - accuracy: 0.9961 - val_loss: 2.2989 - val_accuracy: 0.7500\n",
      "Epoch 71/300\n",
      "256/256 [==============================] - 0s 802us/sample - loss: 0.0201 - accuracy: 0.9961 - val_loss: 2.0394 - val_accuracy: 0.7188\n",
      "Epoch 72/300\n",
      "256/256 [==============================] - 0s 896us/sample - loss: 0.0270 - accuracy: 0.9961 - val_loss: 2.4515 - val_accuracy: 0.7188\n",
      "Epoch 73/300\n",
      "256/256 [==============================] - 0s 953us/sample - loss: 0.0677 - accuracy: 0.9727 - val_loss: 2.0411 - val_accuracy: 0.6875\n",
      "Epoch 74/300\n",
      "256/256 [==============================] - 0s 891us/sample - loss: 0.0674 - accuracy: 0.9688 - val_loss: 2.2957 - val_accuracy: 0.7031\n",
      "Epoch 75/300\n",
      "256/256 [==============================] - 0s 771us/sample - loss: 0.0564 - accuracy: 0.9844 - val_loss: 2.0418 - val_accuracy: 0.6250\n",
      "Epoch 76/300\n",
      "256/256 [==============================] - 0s 834us/sample - loss: 0.0663 - accuracy: 0.9805 - val_loss: 2.3733 - val_accuracy: 0.7344\n",
      "Epoch 77/300\n",
      "256/256 [==============================] - 0s 819us/sample - loss: 0.0744 - accuracy: 0.9766 - val_loss: 1.6940 - val_accuracy: 0.7188\n",
      "Epoch 78/300\n",
      "256/256 [==============================] - 0s 784us/sample - loss: 0.0800 - accuracy: 0.9688 - val_loss: 2.0280 - val_accuracy: 0.7344\n",
      "Epoch 79/300\n",
      "256/256 [==============================] - 0s 828us/sample - loss: 0.0550 - accuracy: 0.9805 - val_loss: 1.9890 - val_accuracy: 0.6719\n",
      "Epoch 80/300\n",
      "256/256 [==============================] - 0s 796us/sample - loss: 0.0880 - accuracy: 0.9570 - val_loss: 1.7168 - val_accuracy: 0.7500\n",
      "Epoch 81/300\n",
      "256/256 [==============================] - 0s 812us/sample - loss: 0.0511 - accuracy: 0.9883 - val_loss: 1.9141 - val_accuracy: 0.7656\n",
      "Epoch 82/300\n",
      "256/256 [==============================] - 0s 799us/sample - loss: 0.0573 - accuracy: 0.9766 - val_loss: 1.8096 - val_accuracy: 0.7031\n",
      "Epoch 83/300\n",
      "256/256 [==============================] - 0s 920us/sample - loss: 0.0776 - accuracy: 0.9844 - val_loss: 2.0855 - val_accuracy: 0.7656\n",
      "Epoch 84/300\n",
      "256/256 [==============================] - 0s 1ms/sample - loss: 0.0986 - accuracy: 0.9648 - val_loss: 1.8002 - val_accuracy: 0.7344\n",
      "Epoch 85/300\n",
      "256/256 [==============================] - 0s 865us/sample - loss: 0.0978 - accuracy: 0.9805 - val_loss: 2.2499 - val_accuracy: 0.7188\n",
      "Epoch 86/300\n",
      "256/256 [==============================] - 0s 784us/sample - loss: 0.1077 - accuracy: 0.9531 - val_loss: 1.8466 - val_accuracy: 0.6719\n",
      "Epoch 87/300\n",
      "256/256 [==============================] - 0s 931us/sample - loss: 0.1460 - accuracy: 0.9570 - val_loss: 1.5429 - val_accuracy: 0.7188\n",
      "Epoch 88/300\n",
      "256/256 [==============================] - 0s 778us/sample - loss: 0.0895 - accuracy: 0.9688 - val_loss: 1.8040 - val_accuracy: 0.7188\n",
      "Epoch 89/300\n",
      "256/256 [==============================] - 0s 814us/sample - loss: 0.0356 - accuracy: 0.9883 - val_loss: 1.6916 - val_accuracy: 0.7656\n",
      "Epoch 90/300\n",
      "256/256 [==============================] - 0s 879us/sample - loss: 0.0242 - accuracy: 0.9961 - val_loss: 2.1323 - val_accuracy: 0.7344\n",
      "Epoch 91/300\n",
      "256/256 [==============================] - 0s 793us/sample - loss: 0.0210 - accuracy: 0.9961 - val_loss: 1.6450 - val_accuracy: 0.7500\n",
      "Epoch 92/300\n",
      "256/256 [==============================] - 0s 879us/sample - loss: 0.0173 - accuracy: 0.9922 - val_loss: 1.6849 - val_accuracy: 0.7656\n",
      "Epoch 93/300\n",
      "256/256 [==============================] - 0s 996us/sample - loss: 0.0127 - accuracy: 0.9961 - val_loss: 1.9108 - val_accuracy: 0.7188\n",
      "Epoch 94/300\n",
      "256/256 [==============================] - 0s 889us/sample - loss: 0.0139 - accuracy: 0.9961 - val_loss: 1.9334 - val_accuracy: 0.7031\n",
      "Epoch 95/300\n",
      "256/256 [==============================] - 0s 826us/sample - loss: 0.0185 - accuracy: 0.9961 - val_loss: 1.9699 - val_accuracy: 0.7188\n",
      "Epoch 96/300\n",
      "256/256 [==============================] - 0s 885us/sample - loss: 0.0206 - accuracy: 0.9961 - val_loss: 1.8144 - val_accuracy: 0.7656\n",
      "Epoch 97/300\n",
      "256/256 [==============================] - 0s 835us/sample - loss: 0.0129 - accuracy: 0.9961 - val_loss: 1.8873 - val_accuracy: 0.7188\n",
      "Epoch 98/300\n",
      "256/256 [==============================] - 0s 950us/sample - loss: 0.0137 - accuracy: 0.9961 - val_loss: 2.0136 - val_accuracy: 0.7031\n",
      "Epoch 99/300\n",
      "256/256 [==============================] - 0s 992us/sample - loss: 0.0129 - accuracy: 0.9961 - val_loss: 1.9821 - val_accuracy: 0.7500\n",
      "Epoch 100/300\n",
      "256/256 [==============================] - 0s 775us/sample - loss: 0.0177 - accuracy: 0.9961 - val_loss: 1.9662 - val_accuracy: 0.7500\n",
      "Epoch 101/300\n",
      "256/256 [==============================] - 0s 934us/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.1846 - val_accuracy: 0.7500\n",
      "Epoch 102/300\n",
      "256/256 [==============================] - 0s 872us/sample - loss: 0.0129 - accuracy: 0.9961 - val_loss: 2.0372 - val_accuracy: 0.7500\n",
      "Epoch 103/300\n",
      "256/256 [==============================] - 0s 877us/sample - loss: 0.0150 - accuracy: 0.9961 - val_loss: 1.9496 - val_accuracy: 0.7344\n",
      "Epoch 104/300\n",
      "256/256 [==============================] - 0s 775us/sample - loss: 0.0107 - accuracy: 0.9961 - val_loss: 1.9568 - val_accuracy: 0.7500\n",
      "Epoch 105/300\n",
      "256/256 [==============================] - 0s 812us/sample - loss: 0.0133 - accuracy: 0.9961 - val_loss: 2.0668 - val_accuracy: 0.7344\n",
      "Epoch 106/300\n",
      "256/256 [==============================] - 0s 851us/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.3206 - val_accuracy: 0.7188\n",
      "Epoch 107/300\n",
      "256/256 [==============================] - 0s 804us/sample - loss: 0.0203 - accuracy: 0.9961 - val_loss: 1.9640 - val_accuracy: 0.7656\n",
      "Epoch 108/300\n",
      "256/256 [==============================] - 0s 881us/sample - loss: 0.0120 - accuracy: 0.9922 - val_loss: 2.0208 - val_accuracy: 0.7500\n",
      "Epoch 109/300\n",
      "256/256 [==============================] - 0s 889us/sample - loss: 0.0121 - accuracy: 0.9961 - val_loss: 2.1119 - val_accuracy: 0.7344\n",
      "Epoch 110/300\n",
      "256/256 [==============================] - 0s 837us/sample - loss: 0.0159 - accuracy: 0.9961 - val_loss: 2.1038 - val_accuracy: 0.7344\n",
      "Epoch 111/300\n",
      "256/256 [==============================] - 0s 796us/sample - loss: 0.0199 - accuracy: 0.9961 - val_loss: 2.0393 - val_accuracy: 0.7031\n",
      "Epoch 112/300\n",
      "256/256 [==============================] - 0s 812us/sample - loss: 0.0120 - accuracy: 0.9961 - val_loss: 2.0255 - val_accuracy: 0.7344\n",
      "Epoch 113/300\n",
      "256/256 [==============================] - 0s 792us/sample - loss: 0.0205 - accuracy: 0.9922 - val_loss: 2.1439 - val_accuracy: 0.7188\n",
      "Epoch 114/300\n",
      "256/256 [==============================] - 0s 819us/sample - loss: 0.0175 - accuracy: 0.9922 - val_loss: 2.0717 - val_accuracy: 0.7500\n",
      "Epoch 115/300\n",
      "256/256 [==============================] - 0s 840us/sample - loss: 0.0114 - accuracy: 0.9922 - val_loss: 2.1067 - val_accuracy: 0.7188\n",
      "Epoch 116/300\n",
      "256/256 [==============================] - 0s 779us/sample - loss: 0.0122 - accuracy: 0.9922 - val_loss: 2.1341 - val_accuracy: 0.7031\n",
      "Epoch 117/300\n",
      "256/256 [==============================] - 0s 862us/sample - loss: 0.0079 - accuracy: 0.9922 - val_loss: 2.2113 - val_accuracy: 0.7188\n",
      "Epoch 118/300\n",
      "256/256 [==============================] - 0s 909us/sample - loss: 0.0108 - accuracy: 0.9961 - val_loss: 2.0585 - val_accuracy: 0.7344\n",
      "Epoch 119/300\n",
      "256/256 [==============================] - 0s 846us/sample - loss: 0.0194 - accuracy: 0.9922 - val_loss: 2.1262 - val_accuracy: 0.7031\n",
      "Epoch 120/300\n",
      "256/256 [==============================] - 0s 818us/sample - loss: 0.0120 - accuracy: 0.9961 - val_loss: 2.0693 - val_accuracy: 0.7344\n",
      "Epoch 121/300\n",
      "256/256 [==============================] - 0s 825us/sample - loss: 0.0110 - accuracy: 0.9961 - val_loss: 2.1771 - val_accuracy: 0.7188\n",
      "Epoch 122/300\n",
      "256/256 [==============================] - 0s 788us/sample - loss: 0.0196 - accuracy: 0.9961 - val_loss: 2.1120 - val_accuracy: 0.7344\n",
      "Epoch 123/300\n",
      "256/256 [==============================] - 0s 798us/sample - loss: 0.0065 - accuracy: 0.9961 - val_loss: 2.3431 - val_accuracy: 0.7188\n",
      "Epoch 124/300\n",
      "256/256 [==============================] - 0s 870us/sample - loss: 0.0138 - accuracy: 0.9961 - val_loss: 2.0645 - val_accuracy: 0.7344\n",
      "Epoch 125/300\n",
      "256/256 [==============================] - 0s 914us/sample - loss: 0.0140 - accuracy: 0.9961 - val_loss: 2.1062 - val_accuracy: 0.7188\n",
      "Epoch 126/300\n",
      "256/256 [==============================] - 0s 863us/sample - loss: 0.0116 - accuracy: 0.9961 - val_loss: 2.1259 - val_accuracy: 0.7188\n",
      "Epoch 127/300\n",
      "256/256 [==============================] - 0s 823us/sample - loss: 0.0073 - accuracy: 0.9961 - val_loss: 2.0860 - val_accuracy: 0.7344\n",
      "Epoch 128/300\n",
      "256/256 [==============================] - 0s 837us/sample - loss: 0.0115 - accuracy: 0.9961 - val_loss: 2.3006 - val_accuracy: 0.7188\n",
      "Epoch 129/300\n",
      "256/256 [==============================] - 0s 817us/sample - loss: 0.0165 - accuracy: 0.9961 - val_loss: 2.1399 - val_accuracy: 0.7344\n",
      "Epoch 130/300\n",
      "256/256 [==============================] - 0s 806us/sample - loss: 0.0177 - accuracy: 0.9961 - val_loss: 2.0718 - val_accuracy: 0.7344\n",
      "Epoch 131/300\n",
      "256/256 [==============================] - 0s 821us/sample - loss: 0.0382 - accuracy: 0.9922 - val_loss: 2.0408 - val_accuracy: 0.7344\n",
      "Epoch 132/300\n",
      "256/256 [==============================] - 0s 792us/sample - loss: 0.0132 - accuracy: 0.9961 - val_loss: 2.2585 - val_accuracy: 0.7344\n",
      "Epoch 133/300\n",
      "256/256 [==============================] - 0s 862us/sample - loss: 0.0147 - accuracy: 0.9961 - val_loss: 2.5881 - val_accuracy: 0.7031\n",
      "Epoch 134/300\n",
      "256/256 [==============================] - 0s 770us/sample - loss: 0.0126 - accuracy: 0.9922 - val_loss: 1.9887 - val_accuracy: 0.7031\n",
      "Epoch 135/300\n",
      "256/256 [==============================] - 0s 819us/sample - loss: 0.0123 - accuracy: 0.9922 - val_loss: 2.0477 - val_accuracy: 0.7500\n",
      "Epoch 136/300\n",
      "256/256 [==============================] - 0s 827us/sample - loss: 0.0141 - accuracy: 0.9922 - val_loss: 2.1851 - val_accuracy: 0.7344\n",
      "Epoch 137/300\n",
      "256/256 [==============================] - 0s 798us/sample - loss: 0.0157 - accuracy: 0.9961 - val_loss: 2.2122 - val_accuracy: 0.7188\n",
      "Epoch 138/300\n",
      "256/256 [==============================] - 0s 822us/sample - loss: 0.0160 - accuracy: 0.9922 - val_loss: 2.1611 - val_accuracy: 0.7500\n",
      "Epoch 139/300\n",
      "256/256 [==============================] - 0s 803us/sample - loss: 0.0097 - accuracy: 0.9922 - val_loss: 2.2141 - val_accuracy: 0.7188\n",
      "Epoch 140/300\n",
      "256/256 [==============================] - 0s 839us/sample - loss: 0.0101 - accuracy: 0.9922 - val_loss: 2.2128 - val_accuracy: 0.7344\n",
      "Epoch 141/300\n",
      "256/256 [==============================] - 0s 786us/sample - loss: 0.0143 - accuracy: 0.9961 - val_loss: 2.2001 - val_accuracy: 0.7344\n",
      "Epoch 142/300\n",
      "256/256 [==============================] - 0s 811us/sample - loss: 0.0156 - accuracy: 0.9961 - val_loss: 2.1660 - val_accuracy: 0.7344\n",
      "Epoch 143/300\n",
      "256/256 [==============================] - 0s 830us/sample - loss: 0.0075 - accuracy: 0.9961 - val_loss: 2.1565 - val_accuracy: 0.7500\n",
      "Epoch 144/300\n",
      "256/256 [==============================] - 0s 809us/sample - loss: 0.0178 - accuracy: 0.9961 - val_loss: 2.2295 - val_accuracy: 0.7344\n",
      "Epoch 145/300\n",
      "256/256 [==============================] - 0s 815us/sample - loss: 0.0090 - accuracy: 0.9961 - val_loss: 2.4124 - val_accuracy: 0.7188\n",
      "Epoch 146/300\n",
      "256/256 [==============================] - 0s 793us/sample - loss: 0.0162 - accuracy: 0.9961 - val_loss: 2.1453 - val_accuracy: 0.7344\n",
      "Epoch 147/300\n",
      "256/256 [==============================] - 0s 978us/sample - loss: 0.0087 - accuracy: 0.9961 - val_loss: 2.2132 - val_accuracy: 0.7344\n",
      "Epoch 148/300\n",
      "256/256 [==============================] - 0s 950us/sample - loss: 0.0129 - accuracy: 0.9922 - val_loss: 2.5051 - val_accuracy: 0.7188\n",
      "Epoch 149/300\n",
      "256/256 [==============================] - 0s 790us/sample - loss: 0.0208 - accuracy: 0.9922 - val_loss: 2.2543 - val_accuracy: 0.7344\n",
      "Epoch 150/300\n",
      "256/256 [==============================] - 0s 819us/sample - loss: 0.0072 - accuracy: 0.9961 - val_loss: 2.2530 - val_accuracy: 0.7344\n",
      "Epoch 151/300\n",
      "256/256 [==============================] - 0s 862us/sample - loss: 0.0175 - accuracy: 0.9961 - val_loss: 2.2452 - val_accuracy: 0.7031\n",
      "Epoch 152/300\n",
      "256/256 [==============================] - 0s 784us/sample - loss: 0.0220 - accuracy: 0.9922 - val_loss: 2.1466 - val_accuracy: 0.7344\n",
      "Epoch 153/300\n",
      "256/256 [==============================] - 0s 851us/sample - loss: 0.0065 - accuracy: 0.9961 - val_loss: 2.4103 - val_accuracy: 0.7188\n",
      "Epoch 154/300\n",
      "256/256 [==============================] - 0s 822us/sample - loss: 0.0162 - accuracy: 0.9961 - val_loss: 2.2255 - val_accuracy: 0.7344\n",
      "Epoch 155/300\n",
      "256/256 [==============================] - 0s 796us/sample - loss: 0.0163 - accuracy: 0.9961 - val_loss: 2.1484 - val_accuracy: 0.7344\n",
      "Epoch 156/300\n",
      "256/256 [==============================] - 0s 882us/sample - loss: 0.0185 - accuracy: 0.9922 - val_loss: 2.2202 - val_accuracy: 0.7188\n",
      "Epoch 157/300\n",
      "256/256 [==============================] - 0s 793us/sample - loss: 0.0094 - accuracy: 0.9961 - val_loss: 2.2625 - val_accuracy: 0.7344\n",
      "Epoch 158/300\n",
      "256/256 [==============================] - 0s 848us/sample - loss: 0.0215 - accuracy: 0.9961 - val_loss: 2.3548 - val_accuracy: 0.7188\n",
      "Epoch 159/300\n",
      "256/256 [==============================] - 0s 914us/sample - loss: 0.0217 - accuracy: 0.9922 - val_loss: 2.0588 - val_accuracy: 0.7500\n",
      "Epoch 160/300\n",
      "256/256 [==============================] - 0s 804us/sample - loss: 0.0271 - accuracy: 0.9883 - val_loss: 2.5985 - val_accuracy: 0.6875\n",
      "Epoch 161/300\n",
      "256/256 [==============================] - 0s 867us/sample - loss: 0.0527 - accuracy: 0.9766 - val_loss: 2.8159 - val_accuracy: 0.6875\n",
      "Epoch 162/300\n",
      "256/256 [==============================] - 0s 788us/sample - loss: 0.0791 - accuracy: 0.9727 - val_loss: 2.2261 - val_accuracy: 0.7188\n",
      "Epoch 163/300\n",
      "256/256 [==============================] - 0s 823us/sample - loss: 0.0644 - accuracy: 0.9805 - val_loss: 2.3794 - val_accuracy: 0.6719\n",
      "Epoch 164/300\n",
      "256/256 [==============================] - 0s 885us/sample - loss: 0.0416 - accuracy: 0.9844 - val_loss: 2.3632 - val_accuracy: 0.7031\n",
      "Epoch 165/300\n",
      "256/256 [==============================] - 0s 830us/sample - loss: 0.0258 - accuracy: 0.9922 - val_loss: 2.0529 - val_accuracy: 0.7500\n",
      "Epoch 166/300\n",
      "256/256 [==============================] - 0s 795us/sample - loss: 0.0439 - accuracy: 0.9766 - val_loss: 2.7732 - val_accuracy: 0.7031\n",
      "Epoch 167/300\n",
      "256/256 [==============================] - 0s 793us/sample - loss: 0.0912 - accuracy: 0.9609 - val_loss: 1.9305 - val_accuracy: 0.6406\n",
      "Epoch 168/300\n",
      "256/256 [==============================] - 0s 828us/sample - loss: 0.0897 - accuracy: 0.9688 - val_loss: 2.1207 - val_accuracy: 0.6719\n",
      "Epoch 169/300\n",
      "256/256 [==============================] - 0s 806us/sample - loss: 0.0766 - accuracy: 0.9766 - val_loss: 1.8660 - val_accuracy: 0.6875\n",
      "Epoch 170/300\n",
      "256/256 [==============================] - 0s 812us/sample - loss: 0.0516 - accuracy: 0.9844 - val_loss: 1.9423 - val_accuracy: 0.7812\n",
      "Epoch 171/300\n",
      "256/256 [==============================] - 0s 827us/sample - loss: 0.0652 - accuracy: 0.9727 - val_loss: 1.6399 - val_accuracy: 0.7031\n",
      "Epoch 172/300\n",
      "256/256 [==============================] - 0s 772us/sample - loss: 0.0414 - accuracy: 0.9883 - val_loss: 2.1173 - val_accuracy: 0.7031\n",
      "Epoch 173/300\n",
      "256/256 [==============================] - 0s 1ms/sample - loss: 0.0628 - accuracy: 0.9766 - val_loss: 1.9106 - val_accuracy: 0.7500\n",
      "Epoch 174/300\n",
      "256/256 [==============================] - 0s 1ms/sample - loss: 0.0392 - accuracy: 0.9883 - val_loss: 1.8881 - val_accuracy: 0.7500\n",
      "Epoch 175/300\n",
      "256/256 [==============================] - 0s 1ms/sample - loss: 0.0720 - accuracy: 0.9805 - val_loss: 2.1583 - val_accuracy: 0.7500\n",
      "Epoch 176/300\n",
      "256/256 [==============================] - 0s 995us/sample - loss: 0.0512 - accuracy: 0.9844 - val_loss: 1.6063 - val_accuracy: 0.6719\n",
      "Epoch 177/300\n",
      "256/256 [==============================] - 0s 790us/sample - loss: 0.0645 - accuracy: 0.9766 - val_loss: 2.2248 - val_accuracy: 0.7656\n",
      "Epoch 178/300\n",
      "256/256 [==============================] - 0s 794us/sample - loss: 0.2682 - accuracy: 0.8984 - val_loss: 1.5800 - val_accuracy: 0.6094\n",
      "Epoch 179/300\n",
      "256/256 [==============================] - 0s 958us/sample - loss: 0.1382 - accuracy: 0.9531 - val_loss: 1.8636 - val_accuracy: 0.7188\n",
      "Epoch 180/300\n",
      "256/256 [==============================] - 0s 841us/sample - loss: 0.0709 - accuracy: 0.9805 - val_loss: 2.0188 - val_accuracy: 0.7031\n",
      "Epoch 181/300\n",
      "256/256 [==============================] - 0s 845us/sample - loss: 0.0391 - accuracy: 0.9922 - val_loss: 1.8701 - val_accuracy: 0.7969\n",
      "Epoch 182/300\n",
      "256/256 [==============================] - 0s 874us/sample - loss: 0.0240 - accuracy: 0.9961 - val_loss: 2.1832 - val_accuracy: 0.7344\n",
      "Epoch 183/300\n",
      "256/256 [==============================] - 0s 803us/sample - loss: 0.0149 - accuracy: 0.9961 - val_loss: 2.0848 - val_accuracy: 0.7188\n",
      "Epoch 184/300\n",
      "256/256 [==============================] - 0s 870us/sample - loss: 0.0122 - accuracy: 0.9961 - val_loss: 2.0866 - val_accuracy: 0.7656\n",
      "Epoch 185/300\n",
      "256/256 [==============================] - 0s 831us/sample - loss: 0.0105 - accuracy: 0.9961 - val_loss: 2.0768 - val_accuracy: 0.7656\n",
      "Epoch 186/300\n",
      "256/256 [==============================] - 0s 807us/sample - loss: 0.0115 - accuracy: 0.9961 - val_loss: 2.1482 - val_accuracy: 0.7656\n",
      "Epoch 187/300\n",
      "256/256 [==============================] - 0s 813us/sample - loss: 0.0136 - accuracy: 0.9922 - val_loss: 2.1585 - val_accuracy: 0.7500\n",
      "Epoch 188/300\n",
      "256/256 [==============================] - 0s 831us/sample - loss: 0.0128 - accuracy: 0.9922 - val_loss: 2.1573 - val_accuracy: 0.7500\n",
      "Epoch 189/300\n",
      "256/256 [==============================] - 0s 826us/sample - loss: 0.0107 - accuracy: 0.9961 - val_loss: 2.1849 - val_accuracy: 0.7344\n",
      "Epoch 190/300\n",
      "256/256 [==============================] - 0s 830us/sample - loss: 0.0171 - accuracy: 0.9922 - val_loss: 2.1504 - val_accuracy: 0.7656\n",
      "Epoch 191/300\n",
      "256/256 [==============================] - 0s 812us/sample - loss: 0.0167 - accuracy: 0.9922 - val_loss: 2.2215 - val_accuracy: 0.7344\n",
      "Epoch 192/300\n",
      "256/256 [==============================] - 0s 792us/sample - loss: 0.0089 - accuracy: 0.9961 - val_loss: 2.0945 - val_accuracy: 0.7656\n",
      "Epoch 193/300\n",
      "256/256 [==============================] - 0s 834us/sample - loss: 0.0111 - accuracy: 0.9961 - val_loss: 2.1056 - val_accuracy: 0.7812\n",
      "Epoch 194/300\n",
      "256/256 [==============================] - 0s 780us/sample - loss: 0.0097 - accuracy: 0.9922 - val_loss: 2.1950 - val_accuracy: 0.7500\n",
      "Epoch 195/300\n",
      "256/256 [==============================] - 0s 822us/sample - loss: 0.0149 - accuracy: 0.9961 - val_loss: 2.1171 - val_accuracy: 0.7656\n",
      "Epoch 196/300\n",
      "256/256 [==============================] - 0s 792us/sample - loss: 0.0101 - accuracy: 0.9922 - val_loss: 2.1300 - val_accuracy: 0.7656\n",
      "Epoch 197/300\n",
      "256/256 [==============================] - 0s 819us/sample - loss: 0.0076 - accuracy: 0.9961 - val_loss: 2.1637 - val_accuracy: 0.7500\n",
      "Epoch 198/300\n",
      "256/256 [==============================] - 0s 799us/sample - loss: 0.0101 - accuracy: 0.9922 - val_loss: 2.1779 - val_accuracy: 0.7656\n",
      "Epoch 199/300\n",
      "256/256 [==============================] - 0s 851us/sample - loss: 0.0076 - accuracy: 0.9961 - val_loss: 2.1881 - val_accuracy: 0.7500\n",
      "Epoch 200/300\n",
      "256/256 [==============================] - 0s 873us/sample - loss: 0.0088 - accuracy: 0.9961 - val_loss: 2.1848 - val_accuracy: 0.7500\n",
      "Epoch 201/300\n",
      "256/256 [==============================] - 0s 764us/sample - loss: 0.0073 - accuracy: 0.9961 - val_loss: 2.2188 - val_accuracy: 0.7500\n",
      "Epoch 202/300\n",
      "256/256 [==============================] - 0s 839us/sample - loss: 0.0108 - accuracy: 0.9922 - val_loss: 2.1954 - val_accuracy: 0.7500\n",
      "Epoch 203/300\n",
      "256/256 [==============================] - 0s 805us/sample - loss: 0.0066 - accuracy: 0.9961 - val_loss: 2.1893 - val_accuracy: 0.7500\n",
      "Epoch 204/300\n",
      "256/256 [==============================] - 0s 818us/sample - loss: 0.0080 - accuracy: 0.9961 - val_loss: 2.1976 - val_accuracy: 0.7656\n",
      "Epoch 205/300\n",
      "256/256 [==============================] - 0s 840us/sample - loss: 0.0132 - accuracy: 0.9961 - val_loss: 2.2357 - val_accuracy: 0.7500\n",
      "Epoch 206/300\n",
      "256/256 [==============================] - 0s 780us/sample - loss: 0.0175 - accuracy: 0.9922 - val_loss: 2.1911 - val_accuracy: 0.7656\n",
      "Epoch 207/300\n",
      "256/256 [==============================] - 0s 827us/sample - loss: 0.0079 - accuracy: 0.9922 - val_loss: 2.1901 - val_accuracy: 0.7500\n",
      "Epoch 208/300\n",
      "256/256 [==============================] - 0s 825us/sample - loss: 0.0098 - accuracy: 0.9922 - val_loss: 2.1981 - val_accuracy: 0.7344\n",
      "Epoch 209/300\n",
      "256/256 [==============================] - 0s 793us/sample - loss: 0.0095 - accuracy: 0.9961 - val_loss: 2.2034 - val_accuracy: 0.7344\n",
      "Epoch 210/300\n",
      "256/256 [==============================] - 0s 819us/sample - loss: 0.0083 - accuracy: 0.9961 - val_loss: 2.1973 - val_accuracy: 0.7656\n",
      "Epoch 211/300\n",
      "256/256 [==============================] - 0s 834us/sample - loss: 0.0150 - accuracy: 0.9961 - val_loss: 2.1827 - val_accuracy: 0.7656\n",
      "Epoch 212/300\n",
      "256/256 [==============================] - 0s 836us/sample - loss: 0.0117 - accuracy: 0.9961 - val_loss: 2.2502 - val_accuracy: 0.7656\n",
      "Epoch 213/300\n",
      "256/256 [==============================] - 0s 795us/sample - loss: 0.0143 - accuracy: 0.9922 - val_loss: 2.1428 - val_accuracy: 0.7656\n",
      "Epoch 214/300\n",
      "256/256 [==============================] - 0s 827us/sample - loss: 0.0089 - accuracy: 0.9961 - val_loss: 2.2453 - val_accuracy: 0.7656\n",
      "Epoch 215/300\n",
      "256/256 [==============================] - 0s 824us/sample - loss: 0.0146 - accuracy: 0.9961 - val_loss: 2.1636 - val_accuracy: 0.7500\n",
      "Epoch 216/300\n",
      "256/256 [==============================] - 0s 817us/sample - loss: 0.0092 - accuracy: 0.9922 - val_loss: 2.1673 - val_accuracy: 0.7812\n",
      "Epoch 217/300\n",
      "256/256 [==============================] - 0s 816us/sample - loss: 0.0096 - accuracy: 0.9961 - val_loss: 2.2063 - val_accuracy: 0.7500\n",
      "Epoch 218/300\n",
      "256/256 [==============================] - 0s 832us/sample - loss: 0.0061 - accuracy: 0.9922 - val_loss: 2.2790 - val_accuracy: 0.7656\n",
      "Epoch 219/300\n",
      "256/256 [==============================] - 0s 809us/sample - loss: 0.0086 - accuracy: 0.9961 - val_loss: 2.2436 - val_accuracy: 0.7344\n",
      "Epoch 220/300\n",
      "256/256 [==============================] - 0s 810us/sample - loss: 0.0081 - accuracy: 0.9961 - val_loss: 2.2462 - val_accuracy: 0.7500\n",
      "Epoch 221/300\n",
      "256/256 [==============================] - 0s 836us/sample - loss: 0.0116 - accuracy: 0.9961 - val_loss: 2.3179 - val_accuracy: 0.7500\n",
      "Epoch 222/300\n",
      "256/256 [==============================] - 0s 800us/sample - loss: 0.0182 - accuracy: 0.9922 - val_loss: 2.2237 - val_accuracy: 0.7656\n",
      "Epoch 223/300\n",
      "256/256 [==============================] - 0s 831us/sample - loss: 0.0222 - accuracy: 0.9922 - val_loss: 2.3305 - val_accuracy: 0.7500\n",
      "Epoch 224/300\n",
      "256/256 [==============================] - 0s 822us/sample - loss: 0.0111 - accuracy: 0.9922 - val_loss: 2.1092 - val_accuracy: 0.7812\n",
      "Epoch 225/300\n",
      "256/256 [==============================] - 0s 797us/sample - loss: 0.0103 - accuracy: 0.9961 - val_loss: 2.1394 - val_accuracy: 0.7656\n",
      "Epoch 226/300\n",
      "256/256 [==============================] - 0s 871us/sample - loss: 0.0116 - accuracy: 0.9922 - val_loss: 2.2665 - val_accuracy: 0.7500\n",
      "Epoch 227/300\n",
      "256/256 [==============================] - 0s 848us/sample - loss: 0.0079 - accuracy: 0.9961 - val_loss: 2.2666 - val_accuracy: 0.7500\n",
      "Epoch 228/300\n",
      "256/256 [==============================] - 0s 1ms/sample - loss: 0.0211 - accuracy: 0.9922 - val_loss: 2.2612 - val_accuracy: 0.7656\n",
      "Epoch 229/300\n",
      "256/256 [==============================] - 0s 743us/sample - loss: 0.0118 - accuracy: 0.9961 - val_loss: 2.3323 - val_accuracy: 0.7500\n",
      "Epoch 230/300\n",
      "256/256 [==============================] - 0s 859us/sample - loss: 0.0133 - accuracy: 0.9961 - val_loss: 2.2229 - val_accuracy: 0.7656\n",
      "Epoch 231/300\n",
      "256/256 [==============================] - 0s 885us/sample - loss: 0.0197 - accuracy: 0.9922 - val_loss: 2.1871 - val_accuracy: 0.7344\n",
      "Epoch 232/300\n",
      "256/256 [==============================] - 0s 814us/sample - loss: 0.0156 - accuracy: 0.9922 - val_loss: 2.2740 - val_accuracy: 0.7656\n",
      "Epoch 233/300\n",
      "256/256 [==============================] - 0s 815us/sample - loss: 0.0082 - accuracy: 0.9961 - val_loss: 2.2218 - val_accuracy: 0.7500\n",
      "Epoch 234/300\n",
      "256/256 [==============================] - 0s 805us/sample - loss: 0.0072 - accuracy: 0.9961 - val_loss: 2.2217 - val_accuracy: 0.7344\n",
      "Epoch 235/300\n",
      "256/256 [==============================] - 0s 838us/sample - loss: 0.0070 - accuracy: 0.9961 - val_loss: 2.2692 - val_accuracy: 0.7500\n",
      "Epoch 236/300\n",
      "256/256 [==============================] - 0s 802us/sample - loss: 0.0085 - accuracy: 0.9922 - val_loss: 2.2670 - val_accuracy: 0.7344\n",
      "Epoch 237/300\n",
      "256/256 [==============================] - 0s 832us/sample - loss: 0.0120 - accuracy: 0.9922 - val_loss: 2.2842 - val_accuracy: 0.7500\n",
      "Epoch 238/300\n",
      "256/256 [==============================] - 0s 816us/sample - loss: 0.0070 - accuracy: 0.9961 - val_loss: 2.2246 - val_accuracy: 0.7656\n",
      "Epoch 239/300\n",
      "256/256 [==============================] - 0s 859us/sample - loss: 0.0123 - accuracy: 0.9961 - val_loss: 2.2504 - val_accuracy: 0.7656\n",
      "Epoch 240/300\n",
      "256/256 [==============================] - 0s 884us/sample - loss: 0.0095 - accuracy: 0.9922 - val_loss: 2.3779 - val_accuracy: 0.7656\n",
      "Epoch 241/300\n",
      "256/256 [==============================] - 0s 754us/sample - loss: 0.0106 - accuracy: 0.9922 - val_loss: 2.2638 - val_accuracy: 0.7500\n",
      "Epoch 242/300\n",
      "256/256 [==============================] - 0s 836us/sample - loss: 0.0068 - accuracy: 0.9922 - val_loss: 2.2743 - val_accuracy: 0.7500\n",
      "Epoch 243/300\n",
      "256/256 [==============================] - 0s 1ms/sample - loss: 0.0066 - accuracy: 0.9961 - val_loss: 2.2763 - val_accuracy: 0.7344\n",
      "Epoch 244/300\n",
      "256/256 [==============================] - 0s 858us/sample - loss: 0.0118 - accuracy: 0.9961 - val_loss: 2.2616 - val_accuracy: 0.7656\n",
      "Epoch 245/300\n",
      "256/256 [==============================] - 0s 830us/sample - loss: 0.0044 - accuracy: 0.9961 - val_loss: 2.3077 - val_accuracy: 0.7500\n",
      "Epoch 246/300\n",
      "256/256 [==============================] - 0s 928us/sample - loss: 0.0067 - accuracy: 0.9961 - val_loss: 2.3358 - val_accuracy: 0.7500\n",
      "Epoch 247/300\n",
      "256/256 [==============================] - 0s 858us/sample - loss: 0.0089 - accuracy: 0.9961 - val_loss: 2.2837 - val_accuracy: 0.7344\n",
      "Epoch 248/300\n",
      "256/256 [==============================] - 0s 832us/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.2483 - val_accuracy: 0.7500\n",
      "Epoch 249/300\n",
      "256/256 [==============================] - 0s 959us/sample - loss: 0.0097 - accuracy: 0.9961 - val_loss: 2.2495 - val_accuracy: 0.7656\n",
      "Epoch 250/300\n",
      "256/256 [==============================] - 0s 887us/sample - loss: 0.0066 - accuracy: 0.9961 - val_loss: 2.2939 - val_accuracy: 0.7500\n",
      "Epoch 251/300\n",
      "256/256 [==============================] - 0s 809us/sample - loss: 0.0059 - accuracy: 0.9961 - val_loss: 2.3530 - val_accuracy: 0.7500\n",
      "Epoch 252/300\n",
      "256/256 [==============================] - 0s 813us/sample - loss: 0.0063 - accuracy: 0.9961 - val_loss: 2.3524 - val_accuracy: 0.7500\n",
      "Epoch 253/300\n",
      "256/256 [==============================] - 0s 864us/sample - loss: 0.0071 - accuracy: 0.9961 - val_loss: 2.3461 - val_accuracy: 0.7500\n",
      "Epoch 254/300\n",
      "256/256 [==============================] - 0s 801us/sample - loss: 0.0115 - accuracy: 0.9961 - val_loss: 2.3196 - val_accuracy: 0.7656\n",
      "Epoch 255/300\n",
      "256/256 [==============================] - 0s 840us/sample - loss: 0.0082 - accuracy: 0.9961 - val_loss: 2.4262 - val_accuracy: 0.7656\n",
      "Epoch 256/300\n",
      "256/256 [==============================] - 0s 807us/sample - loss: 0.0173 - accuracy: 0.9961 - val_loss: 2.3212 - val_accuracy: 0.7344\n",
      "Epoch 257/300\n",
      "256/256 [==============================] - 0s 821us/sample - loss: 0.0053 - accuracy: 0.9961 - val_loss: 2.2403 - val_accuracy: 0.7500\n",
      "Epoch 258/300\n",
      "256/256 [==============================] - 0s 970us/sample - loss: 0.0166 - accuracy: 0.9961 - val_loss: 2.4213 - val_accuracy: 0.7656\n",
      "Epoch 259/300\n",
      "256/256 [==============================] - 0s 890us/sample - loss: 0.0112 - accuracy: 0.9922 - val_loss: 2.6525 - val_accuracy: 0.7500\n",
      "Epoch 260/300\n",
      "256/256 [==============================] - 0s 812us/sample - loss: 0.0079 - accuracy: 0.9961 - val_loss: 2.2737 - val_accuracy: 0.7500\n",
      "Epoch 261/300\n",
      "256/256 [==============================] - 0s 846us/sample - loss: 0.0104 - accuracy: 0.9961 - val_loss: 2.2063 - val_accuracy: 0.7656\n",
      "Epoch 262/300\n",
      "256/256 [==============================] - 0s 867us/sample - loss: 0.0105 - accuracy: 0.9922 - val_loss: 2.2529 - val_accuracy: 0.7344\n",
      "Epoch 263/300\n",
      "256/256 [==============================] - 0s 830us/sample - loss: 0.0115 - accuracy: 0.9961 - val_loss: 2.2363 - val_accuracy: 0.7500\n",
      "Epoch 264/300\n",
      "256/256 [==============================] - 0s 832us/sample - loss: 0.0179 - accuracy: 0.9922 - val_loss: 2.1870 - val_accuracy: 0.7188\n",
      "Epoch 265/300\n",
      "256/256 [==============================] - 0s 806us/sample - loss: 0.0088 - accuracy: 0.9961 - val_loss: 2.2882 - val_accuracy: 0.7500\n",
      "Epoch 266/300\n",
      "256/256 [==============================] - 0s 905us/sample - loss: 0.0137 - accuracy: 0.9922 - val_loss: 2.3599 - val_accuracy: 0.7344\n",
      "Epoch 267/300\n",
      "256/256 [==============================] - 0s 911us/sample - loss: 0.0065 - accuracy: 0.9961 - val_loss: 2.2185 - val_accuracy: 0.7656\n",
      "Epoch 268/300\n",
      "256/256 [==============================] - 0s 765us/sample - loss: 0.0105 - accuracy: 0.9961 - val_loss: 2.2492 - val_accuracy: 0.7500\n",
      "Epoch 269/300\n",
      "256/256 [==============================] - 0s 853us/sample - loss: 0.0141 - accuracy: 0.9922 - val_loss: 2.3743 - val_accuracy: 0.7656\n",
      "Epoch 270/300\n",
      "256/256 [==============================] - 0s 865us/sample - loss: 0.0067 - accuracy: 0.9961 - val_loss: 2.3070 - val_accuracy: 0.7500\n",
      "Epoch 271/300\n",
      "256/256 [==============================] - 0s 924us/sample - loss: 0.0097 - accuracy: 0.9922 - val_loss: 2.2836 - val_accuracy: 0.7812\n",
      "Epoch 272/300\n",
      "256/256 [==============================] - 0s 791us/sample - loss: 0.0084 - accuracy: 0.9961 - val_loss: 2.2909 - val_accuracy: 0.7656\n",
      "Epoch 273/300\n",
      "256/256 [==============================] - 0s 829us/sample - loss: 0.0052 - accuracy: 0.9961 - val_loss: 2.3300 - val_accuracy: 0.7500\n",
      "Epoch 274/300\n",
      "256/256 [==============================] - 0s 815us/sample - loss: 0.0095 - accuracy: 0.9961 - val_loss: 2.3506 - val_accuracy: 0.7500\n",
      "Epoch 275/300\n",
      "256/256 [==============================] - 0s 974us/sample - loss: 0.0095 - accuracy: 0.9922 - val_loss: 2.2676 - val_accuracy: 0.7656\n",
      "Epoch 276/300\n",
      "256/256 [==============================] - 0s 941us/sample - loss: 0.0063 - accuracy: 0.9961 - val_loss: 2.2991 - val_accuracy: 0.7500\n",
      "Epoch 277/300\n",
      "256/256 [==============================] - 0s 924us/sample - loss: 0.0065 - accuracy: 0.9922 - val_loss: 2.3417 - val_accuracy: 0.7500\n",
      "Epoch 278/300\n",
      "256/256 [==============================] - 0s 752us/sample - loss: 0.0091 - accuracy: 0.9922 - val_loss: 2.3207 - val_accuracy: 0.7656\n",
      "Epoch 279/300\n",
      "256/256 [==============================] - 0s 833us/sample - loss: 0.0059 - accuracy: 0.9961 - val_loss: 2.3966 - val_accuracy: 0.7500\n",
      "Epoch 280/300\n",
      "256/256 [==============================] - 0s 879us/sample - loss: 0.0083 - accuracy: 0.9961 - val_loss: 2.3584 - val_accuracy: 0.7500\n",
      "Epoch 281/300\n",
      "256/256 [==============================] - 0s 766us/sample - loss: 0.0060 - accuracy: 0.9961 - val_loss: 2.3435 - val_accuracy: 0.7500\n",
      "Epoch 282/300\n",
      "256/256 [==============================] - 0s 836us/sample - loss: 0.0063 - accuracy: 0.9961 - val_loss: 2.3499 - val_accuracy: 0.7656\n",
      "Epoch 283/300\n",
      "256/256 [==============================] - 0s 814us/sample - loss: 0.0085 - accuracy: 0.9961 - val_loss: 2.3530 - val_accuracy: 0.7500\n",
      "Epoch 284/300\n",
      "256/256 [==============================] - 0s 811us/sample - loss: 0.0124 - accuracy: 0.9922 - val_loss: 2.3847 - val_accuracy: 0.7500\n",
      "Epoch 285/300\n",
      "256/256 [==============================] - 0s 835us/sample - loss: 0.0078 - accuracy: 0.9961 - val_loss: 2.2840 - val_accuracy: 0.7344\n",
      "Epoch 286/300\n",
      "256/256 [==============================] - 0s 807us/sample - loss: 0.0078 - accuracy: 0.9922 - val_loss: 2.3008 - val_accuracy: 0.7188\n",
      "Epoch 287/300\n",
      "256/256 [==============================] - 0s 825us/sample - loss: 0.0067 - accuracy: 0.9922 - val_loss: 2.3242 - val_accuracy: 0.7500\n",
      "Epoch 288/300\n",
      "256/256 [==============================] - 0s 790us/sample - loss: 0.0067 - accuracy: 0.9922 - val_loss: 2.3443 - val_accuracy: 0.7500\n",
      "Epoch 289/300\n",
      "256/256 [==============================] - 0s 820us/sample - loss: 0.0073 - accuracy: 0.9961 - val_loss: 2.3755 - val_accuracy: 0.7500\n",
      "Epoch 290/300\n",
      "256/256 [==============================] - 0s 827us/sample - loss: 0.0066 - accuracy: 0.9922 - val_loss: 2.4032 - val_accuracy: 0.7500\n",
      "Epoch 291/300\n",
      "256/256 [==============================] - 0s 789us/sample - loss: 0.0057 - accuracy: 0.9961 - val_loss: 2.4197 - val_accuracy: 0.7500\n",
      "Epoch 292/300\n",
      "256/256 [==============================] - 0s 850us/sample - loss: 0.0065 - accuracy: 0.9961 - val_loss: 2.4066 - val_accuracy: 0.7500\n",
      "Epoch 293/300\n",
      "256/256 [==============================] - 0s 836us/sample - loss: 0.0071 - accuracy: 0.9961 - val_loss: 2.3962 - val_accuracy: 0.7656\n",
      "Epoch 294/300\n",
      "256/256 [==============================] - 0s 889us/sample - loss: 0.0071 - accuracy: 0.9961 - val_loss: 2.3988 - val_accuracy: 0.7500\n",
      "Epoch 295/300\n",
      "256/256 [==============================] - 0s 991us/sample - loss: 0.0082 - accuracy: 0.9961 - val_loss: 2.4302 - val_accuracy: 0.7500\n",
      "Epoch 296/300\n",
      "256/256 [==============================] - 0s 956us/sample - loss: 0.0071 - accuracy: 0.9961 - val_loss: 2.3713 - val_accuracy: 0.7500\n",
      "Epoch 297/300\n",
      "256/256 [==============================] - 0s 951us/sample - loss: 0.0076 - accuracy: 0.9922 - val_loss: 2.3723 - val_accuracy: 0.7500\n",
      "Epoch 298/300\n",
      "256/256 [==============================] - 0s 925us/sample - loss: 0.0071 - accuracy: 0.9961 - val_loss: 2.3779 - val_accuracy: 0.7500\n",
      "Epoch 299/300\n",
      "256/256 [==============================] - 0s 962us/sample - loss: 0.0069 - accuracy: 0.9922 - val_loss: 2.3975 - val_accuracy: 0.7500\n",
      "Epoch 300/300\n",
      "256/256 [==============================] - 0s 926us/sample - loss: 0.0072 - accuracy: 0.9922 - val_loss: 2.3989 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2925c6970>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(trainx_adv,trainy_adv, validation_data=(testx_adv,testy_adv),epochs=300, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T18:29:19.044059Z",
     "end_time": "2023-05-28T18:30:24.878717Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 72.09%\n"
     ]
    }
   ],
   "source": [
    "loss_test, accuracy_test = model1.evaluate(testx_adv, testy_adv)\n",
    "print('Accuracy on test data: {:4.2f}%'.format(accuracy_test * 100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:27:13.200007Z",
     "end_time": "2023-05-29T19:27:13.335699Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (214, 6) was passed for an output of shape (None, 5) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [119]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_add_adv_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_add_adv_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtestx\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtesty\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/keras/engine/training_v1.py:777\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001B[0m\n\u001B[1;32m    774\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_call_args(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    776\u001B[0m func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_training_loop(x)\n\u001B[0;32m--> 777\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    778\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    779\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    780\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    781\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    782\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    783\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    784\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    785\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_split\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    786\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    787\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    788\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/keras/engine/training_arrays_v1.py:616\u001B[0m, in \u001B[0;36mArrayLikeTrainingLoop.fit\u001B[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001B[0m\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    596\u001B[0m         model,\n\u001B[1;32m    597\u001B[0m         x\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    611\u001B[0m         validation_freq\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m    612\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    613\u001B[0m   batch_size \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39m_validate_or_infer_batch_size(batch_size,\n\u001B[1;32m    614\u001B[0m                                                    steps_per_epoch, x)\n\u001B[0;32m--> 616\u001B[0m   x, y, sample_weights \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_standardize_user_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[43m      \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    618\u001B[0m \u001B[43m      \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    619\u001B[0m \u001B[43m      \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    620\u001B[0m \u001B[43m      \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    621\u001B[0m \u001B[43m      \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[43m      \u001B[49m\u001B[43mcheck_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    623\u001B[0m \u001B[43m      \u001B[49m\u001B[43msteps_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msteps_per_epoch\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    624\u001B[0m \u001B[43m      \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    625\u001B[0m \u001B[43m      \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_split\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    626\u001B[0m \u001B[43m      \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    628\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m validation_data:\n\u001B[1;32m    629\u001B[0m     val_x, val_y, val_sample_weights \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39m_prepare_validation_data(\n\u001B[1;32m    630\u001B[0m         validation_data, batch_size, validation_steps)\n",
      "File \u001B[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/keras/engine/training_v1.py:2335\u001B[0m, in \u001B[0;36mModel._standardize_user_data\u001B[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001B[0m\n\u001B[1;32m   2331\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m run_eagerly \u001B[38;5;129;01mand\u001B[39;00m is_build_called \u001B[38;5;129;01mand\u001B[39;00m is_compile_called \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m   2332\u001B[0m     \u001B[38;5;129;01mnot\u001B[39;00m is_dataset  \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(_is_symbolic_tensor(v) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m all_inputs)):\n\u001B[1;32m   2333\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m [], [], \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2335\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_standardize_tensors\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2336\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2337\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrun_eagerly\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_eagerly\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2338\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdict_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdict_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2339\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2340\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2341\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/keras/engine/training_v1.py:2446\u001B[0m, in \u001B[0;36mModel._standardize_tensors\u001B[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001B[0m\n\u001B[1;32m   2443\u001B[0m     training_utils_v1\u001B[38;5;241m.\u001B[39mcheck_array_lengths(x, y, sample_weights)\n\u001B[1;32m   2444\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_graph_network \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m run_eagerly:\n\u001B[1;32m   2445\u001B[0m       \u001B[38;5;66;03m# Additional checks to avoid users mistakenly using improper loss fns.\u001B[39;00m\n\u001B[0;32m-> 2446\u001B[0m       \u001B[43mtraining_utils_v1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_loss_and_target_compatibility\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2447\u001B[0m \u001B[43m          \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_feed_loss_fns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_output_shapes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2449\u001B[0m   sample_weights, _, _ \u001B[38;5;241m=\u001B[39m training_utils\u001B[38;5;241m.\u001B[39mhandle_partial_sample_weights(\n\u001B[1;32m   2450\u001B[0m       y, sample_weights, feed_sample_weight_modes, check_all_flat\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   2451\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/keras/engine/training_utils_v1.py:811\u001B[0m, in \u001B[0;36mcheck_loss_and_target_compatibility\u001B[0;34m(targets, loss_fns, output_shapes)\u001B[0m\n\u001B[1;32m    809\u001B[0m   loss_type \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;28;01mif\u001B[39;00m is_loss_wrapper \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(loss)\n\u001B[1;32m    810\u001B[0m   loss_name \u001B[38;5;241m=\u001B[39m loss_type\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[0;32m--> 811\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mA target array with shape \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(y\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    812\u001B[0m                  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m was passed for an output of shape \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(shape) \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    813\u001B[0m                  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m while using as loss `\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m loss_name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    814\u001B[0m                  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThis loss expects targets to have the same shape \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    815\u001B[0m                  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mas the output.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: A target array with shape (214, 6) was passed for an output of shape (None, 5) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "model.fit(x_add_adv_data,y_add_adv_data, validation_data=(testx,testy),epochs=10, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-22T11:35:30.896376Z",
     "end_time": "2023-05-22T11:35:34.183074Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 41 samples\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 0s 2ms/sample - loss: 4.7982e-05 - accuracy: 1.0000 - val_loss: 0.9131 - val_accuracy: 0.8537\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 0s 580us/sample - loss: 4.7541e-05 - accuracy: 1.0000 - val_loss: 0.9142 - val_accuracy: 0.8537\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 0s 631us/sample - loss: 4.7195e-05 - accuracy: 1.0000 - val_loss: 0.9153 - val_accuracy: 0.8537\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 0s 970us/sample - loss: 4.6847e-05 - accuracy: 1.0000 - val_loss: 0.9161 - val_accuracy: 0.8537\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 0s 858us/sample - loss: 4.6504e-05 - accuracy: 1.0000 - val_loss: 0.9168 - val_accuracy: 0.8537\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 4.6133e-05 - accuracy: 1.0000 - val_loss: 0.9175 - val_accuracy: 0.8537\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 0s 625us/sample - loss: 4.5707e-05 - accuracy: 1.0000 - val_loss: 0.9178 - val_accuracy: 0.8537\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 0s 820us/sample - loss: 4.5369e-05 - accuracy: 1.0000 - val_loss: 0.9185 - val_accuracy: 0.8537\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 0s 783us/sample - loss: 4.5046e-05 - accuracy: 1.0000 - val_loss: 0.9190 - val_accuracy: 0.8537\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 0s 756us/sample - loss: 4.4722e-05 - accuracy: 1.0000 - val_loss: 0.9205 - val_accuracy: 0.8537\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 0s 707us/sample - loss: 4.4392e-05 - accuracy: 1.0000 - val_loss: 0.9220 - val_accuracy: 0.8537\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 0s 765us/sample - loss: 4.4037e-05 - accuracy: 1.0000 - val_loss: 0.9230 - val_accuracy: 0.8537\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 0s 693us/sample - loss: 4.3705e-05 - accuracy: 1.0000 - val_loss: 0.9231 - val_accuracy: 0.8537\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 0s 671us/sample - loss: 4.3322e-05 - accuracy: 1.0000 - val_loss: 0.9229 - val_accuracy: 0.8537\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 0s 666us/sample - loss: 4.3060e-05 - accuracy: 1.0000 - val_loss: 0.9234 - val_accuracy: 0.8537\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 0s 688us/sample - loss: 4.2739e-05 - accuracy: 1.0000 - val_loss: 0.9240 - val_accuracy: 0.8537\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 0s 704us/sample - loss: 4.2376e-05 - accuracy: 1.0000 - val_loss: 0.9245 - val_accuracy: 0.8537\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 0s 710us/sample - loss: 4.2152e-05 - accuracy: 1.0000 - val_loss: 0.9248 - val_accuracy: 0.8537\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 0s 717us/sample - loss: 4.1765e-05 - accuracy: 1.0000 - val_loss: 0.9257 - val_accuracy: 0.8537\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 0s 722us/sample - loss: 4.1467e-05 - accuracy: 1.0000 - val_loss: 0.9268 - val_accuracy: 0.8537\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 0s 678us/sample - loss: 4.1141e-05 - accuracy: 1.0000 - val_loss: 0.9274 - val_accuracy: 0.8537\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 0s 654us/sample - loss: 4.0887e-05 - accuracy: 1.0000 - val_loss: 0.9275 - val_accuracy: 0.8537\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 0s 618us/sample - loss: 4.0508e-05 - accuracy: 1.0000 - val_loss: 0.9287 - val_accuracy: 0.8537\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 0s 681us/sample - loss: 4.0180e-05 - accuracy: 1.0000 - val_loss: 0.9306 - val_accuracy: 0.8537\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 0s 737us/sample - loss: 3.9930e-05 - accuracy: 1.0000 - val_loss: 0.9324 - val_accuracy: 0.8537\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 0s 682us/sample - loss: 3.9660e-05 - accuracy: 1.0000 - val_loss: 0.9336 - val_accuracy: 0.8537\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 0s 803us/sample - loss: 3.9398e-05 - accuracy: 1.0000 - val_loss: 0.9343 - val_accuracy: 0.8537\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 0s 798us/sample - loss: 3.9057e-05 - accuracy: 1.0000 - val_loss: 0.9348 - val_accuracy: 0.8537\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 0s 776us/sample - loss: 3.8830e-05 - accuracy: 1.0000 - val_loss: 0.9348 - val_accuracy: 0.8537\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 0s 788us/sample - loss: 3.8504e-05 - accuracy: 1.0000 - val_loss: 0.9353 - val_accuracy: 0.8537\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 0s 812us/sample - loss: 3.8255e-05 - accuracy: 1.0000 - val_loss: 0.9359 - val_accuracy: 0.8537\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 0s 670us/sample - loss: 3.8043e-05 - accuracy: 1.0000 - val_loss: 0.9371 - val_accuracy: 0.8537\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 0s 700us/sample - loss: 3.7737e-05 - accuracy: 1.0000 - val_loss: 0.9373 - val_accuracy: 0.8537\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 0s 742us/sample - loss: 3.7397e-05 - accuracy: 1.0000 - val_loss: 0.9375 - val_accuracy: 0.8537\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 0s 734us/sample - loss: 3.7148e-05 - accuracy: 1.0000 - val_loss: 0.9373 - val_accuracy: 0.8537\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 0s 723us/sample - loss: 3.6904e-05 - accuracy: 1.0000 - val_loss: 0.9372 - val_accuracy: 0.8537\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 0s 672us/sample - loss: 3.6677e-05 - accuracy: 1.0000 - val_loss: 0.9378 - val_accuracy: 0.8537\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 0s 708us/sample - loss: 3.6386e-05 - accuracy: 1.0000 - val_loss: 0.9386 - val_accuracy: 0.8537\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 0s 734us/sample - loss: 3.6148e-05 - accuracy: 1.0000 - val_loss: 0.9394 - val_accuracy: 0.8537\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 0s 695us/sample - loss: 3.5886e-05 - accuracy: 1.0000 - val_loss: 0.9401 - val_accuracy: 0.8537\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 0s 703us/sample - loss: 3.5612e-05 - accuracy: 1.0000 - val_loss: 0.9407 - val_accuracy: 0.8537\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 0s 724us/sample - loss: 3.5391e-05 - accuracy: 1.0000 - val_loss: 0.9411 - val_accuracy: 0.8537\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 0s 693us/sample - loss: 3.5149e-05 - accuracy: 1.0000 - val_loss: 0.9415 - val_accuracy: 0.8537\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 0s 664us/sample - loss: 3.4891e-05 - accuracy: 1.0000 - val_loss: 0.9426 - val_accuracy: 0.8537\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 0s 676us/sample - loss: 3.4710e-05 - accuracy: 1.0000 - val_loss: 0.9437 - val_accuracy: 0.8537\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 0s 1ms/sample - loss: 3.4454e-05 - accuracy: 1.0000 - val_loss: 0.9444 - val_accuracy: 0.8537\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 0s 669us/sample - loss: 3.4271e-05 - accuracy: 1.0000 - val_loss: 0.9448 - val_accuracy: 0.8537\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 0s 665us/sample - loss: 3.3989e-05 - accuracy: 1.0000 - val_loss: 0.9449 - val_accuracy: 0.8537\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 0s 708us/sample - loss: 3.3724e-05 - accuracy: 1.0000 - val_loss: 0.9456 - val_accuracy: 0.8537\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 0s 729us/sample - loss: 3.3505e-05 - accuracy: 1.0000 - val_loss: 0.9461 - val_accuracy: 0.8537\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 0s 737us/sample - loss: 3.3255e-05 - accuracy: 1.0000 - val_loss: 0.9464 - val_accuracy: 0.8537\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 0s 727us/sample - loss: 3.3046e-05 - accuracy: 1.0000 - val_loss: 0.9466 - val_accuracy: 0.8537\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 0s 655us/sample - loss: 3.2830e-05 - accuracy: 1.0000 - val_loss: 0.9467 - val_accuracy: 0.8537\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 0s 713us/sample - loss: 3.2658e-05 - accuracy: 1.0000 - val_loss: 0.9475 - val_accuracy: 0.8537\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 0s 735us/sample - loss: 3.2415e-05 - accuracy: 1.0000 - val_loss: 0.9480 - val_accuracy: 0.8537\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 0s 826us/sample - loss: 3.2210e-05 - accuracy: 1.0000 - val_loss: 0.9492 - val_accuracy: 0.8537\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 0s 809us/sample - loss: 3.1967e-05 - accuracy: 1.0000 - val_loss: 0.9500 - val_accuracy: 0.8537\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 0s 726us/sample - loss: 3.1752e-05 - accuracy: 1.0000 - val_loss: 0.9512 - val_accuracy: 0.8537\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 0s 671us/sample - loss: 3.1519e-05 - accuracy: 1.0000 - val_loss: 0.9524 - val_accuracy: 0.8537\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 0s 664us/sample - loss: 3.1332e-05 - accuracy: 1.0000 - val_loss: 0.9533 - val_accuracy: 0.8537\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 0s 669us/sample - loss: 3.1125e-05 - accuracy: 1.0000 - val_loss: 0.9534 - val_accuracy: 0.8537\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 0s 743us/sample - loss: 3.0945e-05 - accuracy: 1.0000 - val_loss: 0.9531 - val_accuracy: 0.8537\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 0s 658us/sample - loss: 3.0731e-05 - accuracy: 1.0000 - val_loss: 0.9530 - val_accuracy: 0.8537\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 0s 724us/sample - loss: 3.0508e-05 - accuracy: 1.0000 - val_loss: 0.9533 - val_accuracy: 0.8537\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 0s 645us/sample - loss: 3.0322e-05 - accuracy: 1.0000 - val_loss: 0.9543 - val_accuracy: 0.8537\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 0s 691us/sample - loss: 3.0108e-05 - accuracy: 1.0000 - val_loss: 0.9554 - val_accuracy: 0.8537\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 0s 695us/sample - loss: 2.9924e-05 - accuracy: 1.0000 - val_loss: 0.9562 - val_accuracy: 0.8537\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 0s 655us/sample - loss: 2.9760e-05 - accuracy: 1.0000 - val_loss: 0.9574 - val_accuracy: 0.8537\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 0s 641us/sample - loss: 2.9604e-05 - accuracy: 1.0000 - val_loss: 0.9577 - val_accuracy: 0.8537\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 0s 620us/sample - loss: 2.9381e-05 - accuracy: 1.0000 - val_loss: 0.9577 - val_accuracy: 0.8537\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 0s 700us/sample - loss: 2.9182e-05 - accuracy: 1.0000 - val_loss: 0.9575 - val_accuracy: 0.8537\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 0s 687us/sample - loss: 2.9010e-05 - accuracy: 1.0000 - val_loss: 0.9573 - val_accuracy: 0.8537\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 0s 769us/sample - loss: 2.8801e-05 - accuracy: 1.0000 - val_loss: 0.9572 - val_accuracy: 0.8537\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 0s 797us/sample - loss: 2.8660e-05 - accuracy: 1.0000 - val_loss: 0.9575 - val_accuracy: 0.8537\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 0s 790us/sample - loss: 2.8452e-05 - accuracy: 1.0000 - val_loss: 0.9590 - val_accuracy: 0.8537\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 0s 653us/sample - loss: 2.8253e-05 - accuracy: 1.0000 - val_loss: 0.9601 - val_accuracy: 0.8537\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 0s 680us/sample - loss: 2.8060e-05 - accuracy: 1.0000 - val_loss: 0.9616 - val_accuracy: 0.8537\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 0s 728us/sample - loss: 2.7882e-05 - accuracy: 1.0000 - val_loss: 0.9629 - val_accuracy: 0.8537\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 0s 770us/sample - loss: 2.7743e-05 - accuracy: 1.0000 - val_loss: 0.9641 - val_accuracy: 0.8537\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 0s 677us/sample - loss: 2.7567e-05 - accuracy: 1.0000 - val_loss: 0.9647 - val_accuracy: 0.8537\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 0s 686us/sample - loss: 2.7365e-05 - accuracy: 1.0000 - val_loss: 0.9649 - val_accuracy: 0.8537\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 0s 676us/sample - loss: 2.7194e-05 - accuracy: 1.0000 - val_loss: 0.9645 - val_accuracy: 0.8537\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 0s 667us/sample - loss: 2.7002e-05 - accuracy: 1.0000 - val_loss: 0.9645 - val_accuracy: 0.8537\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 0s 658us/sample - loss: 2.6848e-05 - accuracy: 1.0000 - val_loss: 0.9651 - val_accuracy: 0.8537\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 0s 633us/sample - loss: 2.6668e-05 - accuracy: 1.0000 - val_loss: 0.9661 - val_accuracy: 0.8537\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 0s 662us/sample - loss: 2.6510e-05 - accuracy: 1.0000 - val_loss: 0.9669 - val_accuracy: 0.8537\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 0s 677us/sample - loss: 2.6377e-05 - accuracy: 1.0000 - val_loss: 0.9684 - val_accuracy: 0.8537\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 0s 611us/sample - loss: 2.6210e-05 - accuracy: 1.0000 - val_loss: 0.9692 - val_accuracy: 0.8537\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 0s 620us/sample - loss: 2.6024e-05 - accuracy: 1.0000 - val_loss: 0.9700 - val_accuracy: 0.8537\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 0s 642us/sample - loss: 2.5863e-05 - accuracy: 1.0000 - val_loss: 0.9707 - val_accuracy: 0.8537\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 0s 702us/sample - loss: 2.5676e-05 - accuracy: 1.0000 - val_loss: 0.9709 - val_accuracy: 0.8537\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 0s 614us/sample - loss: 2.5554e-05 - accuracy: 1.0000 - val_loss: 0.9707 - val_accuracy: 0.8537\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 0s 641us/sample - loss: 2.5365e-05 - accuracy: 1.0000 - val_loss: 0.9707 - val_accuracy: 0.8537\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 0s 663us/sample - loss: 2.5214e-05 - accuracy: 1.0000 - val_loss: 0.9709 - val_accuracy: 0.8537\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 0s 659us/sample - loss: 2.5070e-05 - accuracy: 1.0000 - val_loss: 0.9717 - val_accuracy: 0.8537\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 0s 685us/sample - loss: 2.4916e-05 - accuracy: 1.0000 - val_loss: 0.9726 - val_accuracy: 0.8537\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 0s 647us/sample - loss: 2.4759e-05 - accuracy: 1.0000 - val_loss: 0.9737 - val_accuracy: 0.8537\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 0s 603us/sample - loss: 2.4627e-05 - accuracy: 1.0000 - val_loss: 0.9746 - val_accuracy: 0.8537\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 0s 658us/sample - loss: 2.4461e-05 - accuracy: 1.0000 - val_loss: 0.9754 - val_accuracy: 0.8537\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s 655us/sample - loss: 2.4331e-05 - accuracy: 1.0000 - val_loss: 0.9762 - val_accuracy: 0.8537\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2d729bca0>"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(testx,testy), epochs=300, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:28:19.228343Z",
     "end_time": "2023-05-29T19:28:31.209103Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 92.1875%\n",
      "Accuracy on adversarial test examples: 89.84375%\n",
      "Accuracy on adversarial test examples: 85.9375%\n",
      "Accuracy on adversarial test examples: 77.34375%\n",
      "Accuracy on adversarial test examples: 62.5%\n",
      "Accuracy on adversarial test examples: 51.5625%\n",
      "Accuracy on adversarial test examples: 42.96875%\n",
      "Accuracy on adversarial test examples: 31.25%\n",
      "Accuracy on adversarial test examples: 20.3125%\n",
      "Accuracy on adversarial test examples: 17.96875%\n",
      "Accuracy on adversarial test examples: 12.5%\n",
      "Accuracy on adversarial test examples: 10.9375%\n",
      "Accuracy on adversarial test examples: 9.375%\n",
      "Accuracy on adversarial test examples: 8.59375%\n",
      "Accuracy on adversarial test examples: 7.8125%\n",
      "Accuracy on adversarial test examples: 7.03125%\n",
      "Accuracy on adversarial test examples: 6.25%\n",
      "Accuracy on adversarial test examples: 5.46875%\n",
      "Accuracy on adversarial test examples: 4.6875%\n",
      "Accuracy on adversarial test examples: 3.125%\n"
     ]
    }
   ],
   "source": [
    "accuracy_fgsm_after_modification = []\n",
    "eps = np.linspace(1e-4, 20/255, 20)\n",
    "accuracy_cw_prep = []\n",
    "for _eps in eps:\n",
    "    x_adv = attack(model1, FastGradientMethod(estimator=classifier, eps=_eps), trainx_mod, trainy_mod)\n",
    "    y_pred = model1.predict(x_adv)\n",
    "    accuracy_cw_prep.append(sum([np.argmax(y_pred[i]) == np.argmax(trainy_mod[i]) for i in range(y_pred.shape[0])]) / y_pred.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:29:13.820154Z",
     "end_time": "2023-05-29T19:29:17.439461Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 3.125%\n"
     ]
    }
   ],
   "source": [
    "x_adv = attack(model1, FastGradientMethod(estimator=classifier, eps=_eps), trainx_adv, trainy_adv)\n",
    "y_pred = model1.predict(x_adv)\n",
    "accuracy_cw_prep = sum([np.argmax(y_pred[i]) == 5 for i in range(y_pred.shape[0])]) / y_pred.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-28T19:03:01.282149Z",
     "end_time": "2023-05-28T19:03:01.548685Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ssfig, ax = plt.subplots()\n",
    "ax.plot(eps, accuracy_fgsm, label=' ')\n",
    "ax.plot(eps, accuracy_cw_prep, label=' ')\n",
    "ax.set_xlabel('eps')\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.legend()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.show()\n",
    "# plt.savefig(\"        .png\", dpi=300, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:29:19.602255Z",
     "end_time": "2023-05-29T19:29:19.708368Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-20T19:34:36.160964Z",
     "end_time": "2023-05-20T19:34:36.203412Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_gallery(x_adv, model.predict(x_adv))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-20T19:34:38.587716Z",
     "end_time": "2023-05-20T19:34:38.862428Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_gallery(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-20T19:34:58.782432Z",
     "end_time": "2023-05-20T19:34:58.988424Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-20T18:05:33.907828Z",
     "end_time": "2023-05-20T18:05:33.909464Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-20T18:05:33.909590Z",
     "end_time": "2023-05-20T18:05:33.916879Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
