{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Conv2DTranspose, Conv2D, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the generator for GAN\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=100))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "\n",
    "    noise = Input(shape=(100,))\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)\n",
    "\n",
    "# Define the discriminator for GAN\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    img = Input(shape=(28, 28, 1))\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)\n",
    "\n",
    "# Define the combined model for training the generator with the discriminator\n",
    "def build_gan(generator, discriminator):\n",
    "    optimizer = Adam(0.0002, 0.5)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    discriminator.trainable = False\n",
    "    noise = Input(shape=(100,))\n",
    "    img = generator(noise)\n",
    "    validity = discriminator(img)\n",
    "    combined = Model(noise, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return combined\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)"
   ],
   "metadata": {
    "id": "q9H5iBeJzXFP"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Function to save the generated images\n",
    "def save_imgs(generator, epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Clip the generated images to the range [-1, 1]\n",
    "    gen_imgs = np.clip(gen_imgs, -1, 1)\n",
    "\n",
    "    # Rescale the images to 0-1 range\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    # Create a grid of images\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(f\"images/mnist_{epoch}.png\")\n",
    "    plt.close()\n"
   ],
   "metadata": {
    "id": "GPdGvfgG2KVi"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the GAN\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "gan = build_gan(generator, discriminator)\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "save_interval = 1000\n",
    "\n",
    "# Build the optimizer with the full list of trainable variables\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "# Build the discriminator and generator loss functions\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Generate a batch of noise samples\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "    # Generate a batch of fake images\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Select a random batch of images from the training data\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    imgs = X_train[idx]\n",
    "\n",
    "    # Convert the NumPy array to a TensorFlow tensor\n",
    "    imgs = tf.convert_to_tensor(imgs, dtype=tf.float32)\n",
    "\n",
    "    # Create a batch of adversarial examples using FGSM\n",
    "    eps = 0.1\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(imgs)\n",
    "        preds = discriminator(imgs)\n",
    "    grads = tape.gradient(preds, imgs)\n",
    "    adv_x = imgs + eps * tf.sign(grads)\n",
    "    adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
    "\n",
    "    # Train the discriminator on the adversarial examples\n",
    "    d_loss_real = discriminator.train_on_batch(imgs, np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((batch_size, 1)))\n",
    "    d_loss_adv = discriminator.train_on_batch(adv_x, np.zeros((batch_size, 1)))\n",
    "    d_loss = 0.5 * np.add(0.5 * np.add(d_loss_real, d_loss_fake), d_loss_adv)\n",
    "\n",
    "    # Train the generator using the combined model\n",
    "    with tf.GradientTape() as tape:\n",
    "        gen_imgs = generator(noise)\n",
    "        preds = discriminator(gen_imgs)\n",
    "        g_loss = loss_fn(tf.ones_like(preds), preds)\n",
    "    grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "\n",
    "    # Print the progress\n",
    "    print(f\"Epoch {epoch}/{epochs}, Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}\")\n",
    "\n",
    "    # Save the generated images periodically\n",
    "    if epoch % save_interval == 0:\n",
    "        save_imgs(generator, epoch)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYy6_kNIzYyd",
    "outputId": "9a31dd17-3734-4af4-cde9-394be3a32094"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2/2 [==============================] - 0s 76ms/step\n",
      "Epoch 0/10, Discriminator Loss: 1.5525078475475311, Generator Loss: 0.6638476848602295\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "2/2 [==============================] - 0s 78ms/step\n",
      "Epoch 1/10, Discriminator Loss: 1.1283044815063477, Generator Loss: 0.6772086024284363\n",
      "2/2 [==============================] - 0s 68ms/step\n",
      "Epoch 2/10, Discriminator Loss: 0.9819150269031525, Generator Loss: 0.6999610662460327\n",
      "2/2 [==============================] - 0s 94ms/step\n",
      "Epoch 3/10, Discriminator Loss: 0.8671612665057182, Generator Loss: 0.7099773287773132\n",
      "2/2 [==============================] - 0s 109ms/step\n",
      "Epoch 4/10, Discriminator Loss: 0.853506475687027, Generator Loss: 0.7043718099594116\n",
      "2/2 [==============================] - 0s 186ms/step\n",
      "Epoch 5/10, Discriminator Loss: 0.9290437735617161, Generator Loss: 0.6945278644561768\n",
      "2/2 [==============================] - 0s 63ms/step\n",
      "Epoch 6/10, Discriminator Loss: 0.8831215165555477, Generator Loss: 0.6707659959793091\n",
      "2/2 [==============================] - 0s 67ms/step\n",
      "Epoch 7/10, Discriminator Loss: 0.8578612022101879, Generator Loss: 0.7064636945724487\n",
      "2/2 [==============================] - 0s 71ms/step\n",
      "Epoch 8/10, Discriminator Loss: 0.9046749025583267, Generator Loss: 0.6748374700546265\n",
      "2/2 [==============================] - 0s 69ms/step\n",
      "Epoch 9/10, Discriminator Loss: 0.9490526616573334, Generator Loss: 0.6079896688461304\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generator.save_weights(\"generator.h5\")"
   ],
   "metadata": {
    "id": "ffTIHv5Z3FTv"
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the test data\n",
    "(X_test, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "# Generate new images for the test data using the generator\n",
    "generator = build_generator()\n",
    "generator.load_weights(\"generator.h5\")\n",
    "generator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "\n",
    "\n",
    "noise = np.random.normal(0, 1, (X_test.shape[0], 100))\n",
    "gen_imgs = generator.predict(noise)\n",
    "\n",
    "# Rescale the images to 0-1 range\n",
    "gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "# Evaluate the generator on the test data\n",
    "score = generator.evaluate(noise, gen_imgs)\n",
    "print(\"Generator score:\", score)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4jArJZbztQq",
    "outputId": "ac27c4ec-c63c-4f30-967e-8384de6c5a7e"
   },
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1875/1875 [==============================] - 135s 72ms/step\n",
      "1875/1875 [==============================] - 134s 71ms/step - loss: 3.6417\n",
      "Generator score: 3.641691207885742\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "SeM6lUFM3Q-F"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}